{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0011480655334940944,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 7.653770223293962e-06,
      "grad_norm": 4.914122581481934,
      "learning_rate": 6.666666666666667e-07,
      "loss": 8.303,
      "step": 1
    },
    {
      "epoch": 1.5307540446587925e-05,
      "grad_norm": 4.106173515319824,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 8.2915,
      "step": 2
    },
    {
      "epoch": 2.296131066988189e-05,
      "grad_norm": 4.586526393890381,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 8.5451,
      "step": 3
    },
    {
      "epoch": 3.061508089317585e-05,
      "grad_norm": 4.605806827545166,
      "learning_rate": 2.666666666666667e-06,
      "loss": 8.3551,
      "step": 4
    },
    {
      "epoch": 3.826885111646981e-05,
      "grad_norm": 4.779813766479492,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 8.44,
      "step": 5
    },
    {
      "epoch": 4.592262133976378e-05,
      "grad_norm": 5.068270683288574,
      "learning_rate": 4.000000000000001e-06,
      "loss": 9.235,
      "step": 6
    },
    {
      "epoch": 5.357639156305774e-05,
      "grad_norm": 4.300962924957275,
      "learning_rate": 4.666666666666667e-06,
      "loss": 8.3515,
      "step": 7
    },
    {
      "epoch": 6.12301617863517e-05,
      "grad_norm": 6.471405982971191,
      "learning_rate": 5.333333333333334e-06,
      "loss": 10.4071,
      "step": 8
    },
    {
      "epoch": 6.888393200964566e-05,
      "grad_norm": 4.997831344604492,
      "learning_rate": 6e-06,
      "loss": 8.834,
      "step": 9
    },
    {
      "epoch": 7.653770223293962e-05,
      "grad_norm": 5.541932582855225,
      "learning_rate": 6.666666666666667e-06,
      "loss": 9.5594,
      "step": 10
    },
    {
      "epoch": 8.41914724562336e-05,
      "grad_norm": 5.3805251121521,
      "learning_rate": 7.333333333333333e-06,
      "loss": 9.4003,
      "step": 11
    },
    {
      "epoch": 9.184524267952755e-05,
      "grad_norm": 5.334012508392334,
      "learning_rate": 8.000000000000001e-06,
      "loss": 9.2469,
      "step": 12
    },
    {
      "epoch": 9.949901290282152e-05,
      "grad_norm": 5.680000305175781,
      "learning_rate": 8.666666666666668e-06,
      "loss": 9.3799,
      "step": 13
    },
    {
      "epoch": 0.00010715278312611548,
      "grad_norm": 6.499693870544434,
      "learning_rate": 9.333333333333334e-06,
      "loss": 10.0755,
      "step": 14
    },
    {
      "epoch": 0.00011480655334940944,
      "grad_norm": 5.807558059692383,
      "learning_rate": 1e-05,
      "loss": 9.7534,
      "step": 15
    },
    {
      "epoch": 0.0001224603235727034,
      "grad_norm": 5.530555725097656,
      "learning_rate": 9.925925925925927e-06,
      "loss": 8.6168,
      "step": 16
    },
    {
      "epoch": 0.00013011409379599736,
      "grad_norm": 3.17997145652771,
      "learning_rate": 9.851851851851852e-06,
      "loss": 6.6854,
      "step": 17
    },
    {
      "epoch": 0.00013776786401929132,
      "grad_norm": 5.3292975425720215,
      "learning_rate": 9.777777777777779e-06,
      "loss": 8.6531,
      "step": 18
    },
    {
      "epoch": 0.00014542163424258528,
      "grad_norm": 4.616573810577393,
      "learning_rate": 9.703703703703703e-06,
      "loss": 8.2118,
      "step": 19
    },
    {
      "epoch": 0.00015307540446587924,
      "grad_norm": 4.261197090148926,
      "learning_rate": 9.62962962962963e-06,
      "loss": 7.3419,
      "step": 20
    },
    {
      "epoch": 0.00016072917468917323,
      "grad_norm": 5.016082286834717,
      "learning_rate": 9.555555555555556e-06,
      "loss": 8.1244,
      "step": 21
    },
    {
      "epoch": 0.0001683829449124672,
      "grad_norm": 6.5429368019104,
      "learning_rate": 9.481481481481483e-06,
      "loss": 9.3252,
      "step": 22
    },
    {
      "epoch": 0.00017603671513576115,
      "grad_norm": 4.747773170471191,
      "learning_rate": 9.407407407407408e-06,
      "loss": 7.8916,
      "step": 23
    },
    {
      "epoch": 0.0001836904853590551,
      "grad_norm": 3.7492105960845947,
      "learning_rate": 9.333333333333334e-06,
      "loss": 6.5566,
      "step": 24
    },
    {
      "epoch": 0.00019134425558234907,
      "grad_norm": 6.656974792480469,
      "learning_rate": 9.25925925925926e-06,
      "loss": 9.4832,
      "step": 25
    },
    {
      "epoch": 0.00019899802580564303,
      "grad_norm": 5.176398277282715,
      "learning_rate": 9.185185185185186e-06,
      "loss": 8.1505,
      "step": 26
    },
    {
      "epoch": 0.000206651796028937,
      "grad_norm": 6.8035078048706055,
      "learning_rate": 9.111111111111112e-06,
      "loss": 9.2091,
      "step": 27
    },
    {
      "epoch": 0.00021430556625223095,
      "grad_norm": 5.163696765899658,
      "learning_rate": 9.037037037037037e-06,
      "loss": 7.5524,
      "step": 28
    },
    {
      "epoch": 0.0002219593364755249,
      "grad_norm": 5.270222187042236,
      "learning_rate": 8.962962962962963e-06,
      "loss": 7.842,
      "step": 29
    },
    {
      "epoch": 0.00022961310669881887,
      "grad_norm": 5.733499050140381,
      "learning_rate": 8.888888888888888e-06,
      "loss": 8.6657,
      "step": 30
    },
    {
      "epoch": 0.00023726687692211283,
      "grad_norm": 5.23502779006958,
      "learning_rate": 8.814814814814817e-06,
      "loss": 7.8987,
      "step": 31
    },
    {
      "epoch": 0.0002449206471454068,
      "grad_norm": 4.630730628967285,
      "learning_rate": 8.740740740740741e-06,
      "loss": 7.1228,
      "step": 32
    },
    {
      "epoch": 0.0002525744173687008,
      "grad_norm": 4.345437526702881,
      "learning_rate": 8.666666666666668e-06,
      "loss": 7.3445,
      "step": 33
    },
    {
      "epoch": 0.0002602281875919947,
      "grad_norm": 5.195389270782471,
      "learning_rate": 8.592592592592593e-06,
      "loss": 7.692,
      "step": 34
    },
    {
      "epoch": 0.0002678819578152887,
      "grad_norm": 6.0369648933410645,
      "learning_rate": 8.518518518518519e-06,
      "loss": 8.2675,
      "step": 35
    },
    {
      "epoch": 0.00027553572803858264,
      "grad_norm": 5.991506576538086,
      "learning_rate": 8.444444444444446e-06,
      "loss": 8.0948,
      "step": 36
    },
    {
      "epoch": 0.0002831894982618766,
      "grad_norm": 5.727706432342529,
      "learning_rate": 8.37037037037037e-06,
      "loss": 8.0787,
      "step": 37
    },
    {
      "epoch": 0.00029084326848517056,
      "grad_norm": 6.803436279296875,
      "learning_rate": 8.296296296296297e-06,
      "loss": 8.6768,
      "step": 38
    },
    {
      "epoch": 0.00029849703870846455,
      "grad_norm": 5.808382987976074,
      "learning_rate": 8.222222222222222e-06,
      "loss": 7.7225,
      "step": 39
    },
    {
      "epoch": 0.0003061508089317585,
      "grad_norm": 5.463085651397705,
      "learning_rate": 8.148148148148148e-06,
      "loss": 7.5067,
      "step": 40
    },
    {
      "epoch": 0.00031380457915505247,
      "grad_norm": 5.968760967254639,
      "learning_rate": 8.074074074074075e-06,
      "loss": 7.9161,
      "step": 41
    },
    {
      "epoch": 0.00032145834937834646,
      "grad_norm": NaN,
      "learning_rate": 8.074074074074075e-06,
      "loss": 8.1011,
      "step": 42
    },
    {
      "epoch": 0.0003291121196016404,
      "grad_norm": 5.110297203063965,
      "learning_rate": 8.000000000000001e-06,
      "loss": 7.1256,
      "step": 43
    },
    {
      "epoch": 0.0003367658898249344,
      "grad_norm": 5.460613250732422,
      "learning_rate": 7.925925925925926e-06,
      "loss": 7.3246,
      "step": 44
    },
    {
      "epoch": 0.0003444196600482283,
      "grad_norm": 5.898072242736816,
      "learning_rate": 7.851851851851853e-06,
      "loss": 7.8548,
      "step": 45
    },
    {
      "epoch": 0.0003520734302715223,
      "grad_norm": 5.975387096405029,
      "learning_rate": 7.77777777777778e-06,
      "loss": 7.3731,
      "step": 46
    },
    {
      "epoch": 0.00035972720049481623,
      "grad_norm": 5.117428302764893,
      "learning_rate": 7.703703703703704e-06,
      "loss": 7.0464,
      "step": 47
    },
    {
      "epoch": 0.0003673809707181102,
      "grad_norm": 5.755771636962891,
      "learning_rate": 7.62962962962963e-06,
      "loss": 7.3814,
      "step": 48
    },
    {
      "epoch": 0.00037503474094140415,
      "grad_norm": 6.621214389801025,
      "learning_rate": 7.555555555555556e-06,
      "loss": 7.9231,
      "step": 49
    },
    {
      "epoch": 0.00038268851116469814,
      "grad_norm": 5.408539772033691,
      "learning_rate": 7.481481481481482e-06,
      "loss": 7.1117,
      "step": 50
    },
    {
      "epoch": 0.0003903422813879921,
      "grad_norm": 5.642579555511475,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 7.1625,
      "step": 51
    },
    {
      "epoch": 0.00039799605161128606,
      "grad_norm": 6.34633207321167,
      "learning_rate": 7.333333333333333e-06,
      "loss": 7.2217,
      "step": 52
    },
    {
      "epoch": 0.00040564982183458,
      "grad_norm": 5.812069892883301,
      "learning_rate": 7.2592592592592605e-06,
      "loss": 7.394,
      "step": 53
    },
    {
      "epoch": 0.000413303592057874,
      "grad_norm": NaN,
      "learning_rate": 7.2592592592592605e-06,
      "loss": 7.4181,
      "step": 54
    },
    {
      "epoch": 0.00042095736228116797,
      "grad_norm": 5.618387222290039,
      "learning_rate": 7.185185185185186e-06,
      "loss": 7.0971,
      "step": 55
    },
    {
      "epoch": 0.0004286111325044619,
      "grad_norm": 5.955078601837158,
      "learning_rate": 7.111111111111112e-06,
      "loss": 7.068,
      "step": 56
    },
    {
      "epoch": 0.0004362649027277559,
      "grad_norm": 6.2190752029418945,
      "learning_rate": 7.0370370370370375e-06,
      "loss": 7.3285,
      "step": 57
    },
    {
      "epoch": 0.0004439186729510498,
      "grad_norm": 6.063862323760986,
      "learning_rate": 6.962962962962964e-06,
      "loss": 7.4479,
      "step": 58
    },
    {
      "epoch": 0.0004515724431743438,
      "grad_norm": 4.723182678222656,
      "learning_rate": 6.88888888888889e-06,
      "loss": 6.7654,
      "step": 59
    },
    {
      "epoch": 0.00045922621339763775,
      "grad_norm": 4.796474456787109,
      "learning_rate": 6.814814814814815e-06,
      "loss": 6.5648,
      "step": 60
    },
    {
      "epoch": 0.00046687998362093173,
      "grad_norm": 5.125892162322998,
      "learning_rate": 6.740740740740741e-06,
      "loss": 6.6181,
      "step": 61
    },
    {
      "epoch": 0.00047453375384422567,
      "grad_norm": 5.393930912017822,
      "learning_rate": 6.666666666666667e-06,
      "loss": 6.6356,
      "step": 62
    },
    {
      "epoch": 0.00048218752406751966,
      "grad_norm": 4.747994422912598,
      "learning_rate": 6.592592592592592e-06,
      "loss": 6.2953,
      "step": 63
    },
    {
      "epoch": 0.0004898412942908136,
      "grad_norm": 5.898232936859131,
      "learning_rate": 6.51851851851852e-06,
      "loss": 7.1058,
      "step": 64
    },
    {
      "epoch": 0.0004974950645141075,
      "grad_norm": 5.573493003845215,
      "learning_rate": 6.444444444444445e-06,
      "loss": 6.8395,
      "step": 65
    },
    {
      "epoch": 0.0005051488347374016,
      "grad_norm": 5.055373668670654,
      "learning_rate": 6.370370370370371e-06,
      "loss": 6.7017,
      "step": 66
    },
    {
      "epoch": 0.0005128026049606955,
      "grad_norm": 5.677459716796875,
      "learning_rate": 6.296296296296297e-06,
      "loss": 6.8297,
      "step": 67
    },
    {
      "epoch": 0.0005204563751839894,
      "grad_norm": 5.001908302307129,
      "learning_rate": 6.222222222222223e-06,
      "loss": 6.4863,
      "step": 68
    },
    {
      "epoch": 0.0005281101454072835,
      "grad_norm": 5.986841201782227,
      "learning_rate": 6.148148148148149e-06,
      "loss": 6.9306,
      "step": 69
    },
    {
      "epoch": 0.0005357639156305774,
      "grad_norm": 4.351851940155029,
      "learning_rate": 6.0740740740740745e-06,
      "loss": 5.9721,
      "step": 70
    },
    {
      "epoch": 0.0005434176858538713,
      "grad_norm": 5.399590492248535,
      "learning_rate": 6e-06,
      "loss": 6.5415,
      "step": 71
    },
    {
      "epoch": 0.0005510714560771653,
      "grad_norm": 4.37050199508667,
      "learning_rate": 5.925925925925926e-06,
      "loss": 6.0908,
      "step": 72
    },
    {
      "epoch": 0.0005587252263004593,
      "grad_norm": 5.043421745300293,
      "learning_rate": 5.8518518518518515e-06,
      "loss": 6.5465,
      "step": 73
    },
    {
      "epoch": 0.0005663789965237532,
      "grad_norm": 5.020814895629883,
      "learning_rate": 5.777777777777778e-06,
      "loss": 6.4116,
      "step": 74
    },
    {
      "epoch": 0.0005740327667470472,
      "grad_norm": 4.505062103271484,
      "learning_rate": 5.7037037037037045e-06,
      "loss": 6.29,
      "step": 75
    },
    {
      "epoch": 0.0005816865369703411,
      "grad_norm": 5.0331292152404785,
      "learning_rate": 5.62962962962963e-06,
      "loss": 6.5421,
      "step": 76
    },
    {
      "epoch": 0.0005893403071936352,
      "grad_norm": 4.157555103302002,
      "learning_rate": 5.555555555555557e-06,
      "loss": 6.0529,
      "step": 77
    },
    {
      "epoch": 0.0005969940774169291,
      "grad_norm": 5.222238540649414,
      "learning_rate": 5.481481481481482e-06,
      "loss": 6.6469,
      "step": 78
    },
    {
      "epoch": 0.000604647847640223,
      "grad_norm": 4.665149688720703,
      "learning_rate": 5.407407407407408e-06,
      "loss": 6.5831,
      "step": 79
    },
    {
      "epoch": 0.000612301617863517,
      "grad_norm": 5.100517272949219,
      "learning_rate": 5.333333333333334e-06,
      "loss": 6.5081,
      "step": 80
    },
    {
      "epoch": 0.000619955388086811,
      "grad_norm": 3.765774726867676,
      "learning_rate": 5.259259259259259e-06,
      "loss": 6.2121,
      "step": 81
    },
    {
      "epoch": 0.0006276091583101049,
      "grad_norm": 3.6263539791107178,
      "learning_rate": 5.185185185185185e-06,
      "loss": 5.8025,
      "step": 82
    },
    {
      "epoch": 0.0006352629285333989,
      "grad_norm": 4.124402046203613,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 6.4114,
      "step": 83
    },
    {
      "epoch": 0.0006429166987566929,
      "grad_norm": 3.9898059368133545,
      "learning_rate": 5.037037037037037e-06,
      "loss": 5.9249,
      "step": 84
    },
    {
      "epoch": 0.0006505704689799868,
      "grad_norm": 2.743068218231201,
      "learning_rate": 4.962962962962964e-06,
      "loss": 4.7508,
      "step": 85
    },
    {
      "epoch": 0.0006582242392032808,
      "grad_norm": 3.7696239948272705,
      "learning_rate": 4.888888888888889e-06,
      "loss": 5.8384,
      "step": 86
    },
    {
      "epoch": 0.0006658780094265747,
      "grad_norm": 4.254659175872803,
      "learning_rate": 4.814814814814815e-06,
      "loss": 5.9986,
      "step": 87
    },
    {
      "epoch": 0.0006735317796498688,
      "grad_norm": 3.334810733795166,
      "learning_rate": 4.7407407407407415e-06,
      "loss": 5.4962,
      "step": 88
    },
    {
      "epoch": 0.0006811855498731627,
      "grad_norm": 3.990863084793091,
      "learning_rate": 4.666666666666667e-06,
      "loss": 5.9401,
      "step": 89
    },
    {
      "epoch": 0.0006888393200964566,
      "grad_norm": 3.4022581577301025,
      "learning_rate": 4.592592592592593e-06,
      "loss": 5.3713,
      "step": 90
    },
    {
      "epoch": 0.0006964930903197506,
      "grad_norm": 3.8497114181518555,
      "learning_rate": 4.5185185185185185e-06,
      "loss": 5.6035,
      "step": 91
    },
    {
      "epoch": 0.0007041468605430446,
      "grad_norm": 3.5570859909057617,
      "learning_rate": 4.444444444444444e-06,
      "loss": 5.8795,
      "step": 92
    },
    {
      "epoch": 0.0007118006307663385,
      "grad_norm": 4.035972595214844,
      "learning_rate": 4.370370370370371e-06,
      "loss": 5.93,
      "step": 93
    },
    {
      "epoch": 0.0007194544009896325,
      "grad_norm": 4.05921745300293,
      "learning_rate": 4.296296296296296e-06,
      "loss": 6.0516,
      "step": 94
    },
    {
      "epoch": 0.0007271081712129265,
      "grad_norm": 3.772160768508911,
      "learning_rate": 4.222222222222223e-06,
      "loss": 5.9764,
      "step": 95
    },
    {
      "epoch": 0.0007347619414362204,
      "grad_norm": 3.3748414516448975,
      "learning_rate": 4.1481481481481485e-06,
      "loss": 5.9738,
      "step": 96
    },
    {
      "epoch": 0.0007424157116595144,
      "grad_norm": 3.999753475189209,
      "learning_rate": 4.074074074074074e-06,
      "loss": 6.2136,
      "step": 97
    },
    {
      "epoch": 0.0007500694818828083,
      "grad_norm": 2.992340564727783,
      "learning_rate": 4.000000000000001e-06,
      "loss": 5.4391,
      "step": 98
    },
    {
      "epoch": 0.0007577232521061023,
      "grad_norm": 3.478863477706909,
      "learning_rate": 3.925925925925926e-06,
      "loss": 5.7702,
      "step": 99
    },
    {
      "epoch": 0.0007653770223293963,
      "grad_norm": 3.7847518920898438,
      "learning_rate": 3.851851851851852e-06,
      "loss": 6.0847,
      "step": 100
    },
    {
      "epoch": 0.0007730307925526902,
      "grad_norm": 3.8556759357452393,
      "learning_rate": 3.777777777777778e-06,
      "loss": 5.9924,
      "step": 101
    },
    {
      "epoch": 0.0007806845627759841,
      "grad_norm": 3.3205361366271973,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 5.8544,
      "step": 102
    },
    {
      "epoch": 0.0007883383329992782,
      "grad_norm": 3.887319803237915,
      "learning_rate": 3.6296296296296302e-06,
      "loss": 6.1714,
      "step": 103
    },
    {
      "epoch": 0.0007959921032225721,
      "grad_norm": 3.2147443294525146,
      "learning_rate": 3.555555555555556e-06,
      "loss": 5.7103,
      "step": 104
    },
    {
      "epoch": 0.0008036458734458661,
      "grad_norm": 2.7577788829803467,
      "learning_rate": 3.481481481481482e-06,
      "loss": 5.5731,
      "step": 105
    },
    {
      "epoch": 0.00081129964366916,
      "grad_norm": 4.1066813468933105,
      "learning_rate": 3.4074074074074077e-06,
      "loss": 5.898,
      "step": 106
    },
    {
      "epoch": 0.000818953413892454,
      "grad_norm": 3.871016263961792,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 6.0247,
      "step": 107
    },
    {
      "epoch": 0.000826607184115748,
      "grad_norm": 4.025548934936523,
      "learning_rate": 3.25925925925926e-06,
      "loss": 6.0922,
      "step": 108
    },
    {
      "epoch": 0.0008342609543390419,
      "grad_norm": 3.630462169647217,
      "learning_rate": 3.1851851851851855e-06,
      "loss": 5.8186,
      "step": 109
    },
    {
      "epoch": 0.0008419147245623359,
      "grad_norm": 3.6633527278900146,
      "learning_rate": 3.1111111111111116e-06,
      "loss": 6.0076,
      "step": 110
    },
    {
      "epoch": 0.0008495684947856299,
      "grad_norm": 3.505941152572632,
      "learning_rate": 3.0370370370370372e-06,
      "loss": 5.8289,
      "step": 111
    },
    {
      "epoch": 0.0008572222650089238,
      "grad_norm": 3.1208062171936035,
      "learning_rate": 2.962962962962963e-06,
      "loss": 5.302,
      "step": 112
    },
    {
      "epoch": 0.0008648760352322177,
      "grad_norm": 3.237488269805908,
      "learning_rate": 2.888888888888889e-06,
      "loss": 5.6574,
      "step": 113
    },
    {
      "epoch": 0.0008725298054555118,
      "grad_norm": 3.0538713932037354,
      "learning_rate": 2.814814814814815e-06,
      "loss": 5.3308,
      "step": 114
    },
    {
      "epoch": 0.0008801835756788057,
      "grad_norm": 2.5305652618408203,
      "learning_rate": 2.740740740740741e-06,
      "loss": 4.8899,
      "step": 115
    },
    {
      "epoch": 0.0008878373459020997,
      "grad_norm": 2.9985098838806152,
      "learning_rate": 2.666666666666667e-06,
      "loss": 5.4036,
      "step": 116
    },
    {
      "epoch": 0.0008954911161253936,
      "grad_norm": 3.740753173828125,
      "learning_rate": 2.5925925925925925e-06,
      "loss": 5.8347,
      "step": 117
    },
    {
      "epoch": 0.0009031448863486876,
      "grad_norm": 3.9635252952575684,
      "learning_rate": 2.5185185185185186e-06,
      "loss": 5.9302,
      "step": 118
    },
    {
      "epoch": 0.0009107986565719816,
      "grad_norm": 3.515618085861206,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 5.8363,
      "step": 119
    },
    {
      "epoch": 0.0009184524267952755,
      "grad_norm": 2.8039231300354004,
      "learning_rate": 2.3703703703703707e-06,
      "loss": 5.5163,
      "step": 120
    },
    {
      "epoch": 0.0009261061970185695,
      "grad_norm": 3.6498289108276367,
      "learning_rate": 2.2962962962962964e-06,
      "loss": 5.5471,
      "step": 121
    },
    {
      "epoch": 0.0009337599672418635,
      "grad_norm": 3.4615838527679443,
      "learning_rate": 2.222222222222222e-06,
      "loss": 5.5162,
      "step": 122
    },
    {
      "epoch": 0.0009414137374651574,
      "grad_norm": 3.1166839599609375,
      "learning_rate": 2.148148148148148e-06,
      "loss": 5.7388,
      "step": 123
    },
    {
      "epoch": 0.0009490675076884513,
      "grad_norm": 2.5952022075653076,
      "learning_rate": 2.0740740740740742e-06,
      "loss": 4.8224,
      "step": 124
    },
    {
      "epoch": 0.0009567212779117454,
      "grad_norm": 3.30523943901062,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 5.8759,
      "step": 125
    },
    {
      "epoch": 0.0009643750481350393,
      "grad_norm": 3.076439380645752,
      "learning_rate": 1.925925925925926e-06,
      "loss": 5.6282,
      "step": 126
    },
    {
      "epoch": 0.0009720288183583332,
      "grad_norm": 2.8034565448760986,
      "learning_rate": 1.8518518518518519e-06,
      "loss": 5.3192,
      "step": 127
    },
    {
      "epoch": 0.0009796825885816272,
      "grad_norm": 3.195308208465576,
      "learning_rate": 1.777777777777778e-06,
      "loss": 5.5272,
      "step": 128
    },
    {
      "epoch": 0.0009873363588049212,
      "grad_norm": 2.821117401123047,
      "learning_rate": 1.7037037037037038e-06,
      "loss": 5.3246,
      "step": 129
    },
    {
      "epoch": 0.000994990129028215,
      "grad_norm": 3.339019536972046,
      "learning_rate": 1.62962962962963e-06,
      "loss": 5.5062,
      "step": 130
    },
    {
      "epoch": 0.001002643899251509,
      "grad_norm": 2.916844367980957,
      "learning_rate": 1.5555555555555558e-06,
      "loss": 5.374,
      "step": 131
    },
    {
      "epoch": 0.0010102976694748031,
      "grad_norm": 3.1544156074523926,
      "learning_rate": 1.4814814814814815e-06,
      "loss": 5.9302,
      "step": 132
    },
    {
      "epoch": 0.001017951439698097,
      "grad_norm": 2.510974645614624,
      "learning_rate": 1.4074074074074075e-06,
      "loss": 4.9498,
      "step": 133
    },
    {
      "epoch": 0.001025605209921391,
      "grad_norm": 3.5516762733459473,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 5.5681,
      "step": 134
    },
    {
      "epoch": 0.001033258980144685,
      "grad_norm": 2.638446807861328,
      "learning_rate": 1.2592592592592593e-06,
      "loss": 4.9176,
      "step": 135
    },
    {
      "epoch": 0.0010409127503679789,
      "grad_norm": 3.412961483001709,
      "learning_rate": 1.1851851851851854e-06,
      "loss": 5.738,
      "step": 136
    },
    {
      "epoch": 0.001048566520591273,
      "grad_norm": 2.42824387550354,
      "learning_rate": 1.111111111111111e-06,
      "loss": 4.6651,
      "step": 137
    },
    {
      "epoch": 0.001056220290814567,
      "grad_norm": 3.3300085067749023,
      "learning_rate": 1.0370370370370371e-06,
      "loss": 5.5436,
      "step": 138
    },
    {
      "epoch": 0.0010638740610378608,
      "grad_norm": 2.947559118270874,
      "learning_rate": 9.62962962962963e-07,
      "loss": 5.3414,
      "step": 139
    },
    {
      "epoch": 0.0010715278312611548,
      "grad_norm": 2.7362563610076904,
      "learning_rate": 8.88888888888889e-07,
      "loss": 5.0102,
      "step": 140
    },
    {
      "epoch": 0.0010791816014844486,
      "grad_norm": 2.9919047355651855,
      "learning_rate": 8.14814814814815e-07,
      "loss": 5.3019,
      "step": 141
    },
    {
      "epoch": 0.0010868353717077427,
      "grad_norm": 2.878725290298462,
      "learning_rate": 7.407407407407407e-07,
      "loss": 5.2664,
      "step": 142
    },
    {
      "epoch": 0.0010944891419310367,
      "grad_norm": 2.5640408992767334,
      "learning_rate": 6.666666666666667e-07,
      "loss": 5.3651,
      "step": 143
    },
    {
      "epoch": 0.0011021429121543305,
      "grad_norm": 3.2087221145629883,
      "learning_rate": 5.925925925925927e-07,
      "loss": 5.7232,
      "step": 144
    },
    {
      "epoch": 0.0011097966823776246,
      "grad_norm": 3.0045506954193115,
      "learning_rate": 5.185185185185186e-07,
      "loss": 5.5852,
      "step": 145
    },
    {
      "epoch": 0.0011174504526009186,
      "grad_norm": 3.440906286239624,
      "learning_rate": 4.444444444444445e-07,
      "loss": 5.7187,
      "step": 146
    },
    {
      "epoch": 0.0011251042228242125,
      "grad_norm": 3.2752785682678223,
      "learning_rate": 3.7037037037037036e-07,
      "loss": 5.2881,
      "step": 147
    },
    {
      "epoch": 0.0011327579930475065,
      "grad_norm": 2.7218427658081055,
      "learning_rate": 2.9629629629629634e-07,
      "loss": 5.4289,
      "step": 148
    },
    {
      "epoch": 0.0011404117632708005,
      "grad_norm": 3.203244686126709,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 5.4034,
      "step": 149
    },
    {
      "epoch": 0.0011480655334940944,
      "grad_norm": 3.2699527740478516,
      "learning_rate": 1.4814814814814817e-07,
      "loss": 5.5927,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 150,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1131677770655232.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
