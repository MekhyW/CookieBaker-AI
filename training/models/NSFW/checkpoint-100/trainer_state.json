{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.001778851017891906,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.778851017891906e-05,
      "grad_norm": 5.640249252319336,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 10.0472,
      "step": 1
    },
    {
      "epoch": 3.557702035783812e-05,
      "grad_norm": 5.3359575271606445,
      "learning_rate": 4.000000000000001e-06,
      "loss": 9.9103,
      "step": 2
    },
    {
      "epoch": 5.3365530536757174e-05,
      "grad_norm": 5.645761966705322,
      "learning_rate": 6e-06,
      "loss": 9.9043,
      "step": 3
    },
    {
      "epoch": 7.115404071567624e-05,
      "grad_norm": 6.25145149230957,
      "learning_rate": 8.000000000000001e-06,
      "loss": 10.6738,
      "step": 4
    },
    {
      "epoch": 8.89425508945953e-05,
      "grad_norm": 4.865102767944336,
      "learning_rate": 1e-05,
      "loss": 9.0942,
      "step": 5
    },
    {
      "epoch": 0.00010673106107351435,
      "grad_norm": 6.099635601043701,
      "learning_rate": 9.894736842105264e-06,
      "loss": 10.5909,
      "step": 6
    },
    {
      "epoch": 0.00012451957125243342,
      "grad_norm": 6.482822418212891,
      "learning_rate": 9.789473684210527e-06,
      "loss": 10.7541,
      "step": 7
    },
    {
      "epoch": 0.00014230808143135247,
      "grad_norm": 6.0161590576171875,
      "learning_rate": 9.68421052631579e-06,
      "loss": 10.1237,
      "step": 8
    },
    {
      "epoch": 0.00016009659161027152,
      "grad_norm": 5.7317585945129395,
      "learning_rate": 9.578947368421054e-06,
      "loss": 9.6602,
      "step": 9
    },
    {
      "epoch": 0.0001778851017891906,
      "grad_norm": 3.9530227184295654,
      "learning_rate": 9.473684210526315e-06,
      "loss": 7.3626,
      "step": 10
    },
    {
      "epoch": 0.00019567361196810965,
      "grad_norm": 6.8787522315979,
      "learning_rate": 9.36842105263158e-06,
      "loss": 10.7523,
      "step": 11
    },
    {
      "epoch": 0.0002134621221470287,
      "grad_norm": 6.155326843261719,
      "learning_rate": 9.263157894736842e-06,
      "loss": 9.9793,
      "step": 12
    },
    {
      "epoch": 0.00023125063232594777,
      "grad_norm": 6.502983093261719,
      "learning_rate": 9.157894736842105e-06,
      "loss": 10.1341,
      "step": 13
    },
    {
      "epoch": 0.00024903914250486685,
      "grad_norm": 5.242326259613037,
      "learning_rate": 9.05263157894737e-06,
      "loss": 9.0086,
      "step": 14
    },
    {
      "epoch": 0.0002668276526837859,
      "grad_norm": 5.857893943786621,
      "learning_rate": 8.947368421052632e-06,
      "loss": 9.5701,
      "step": 15
    },
    {
      "epoch": 0.00028461616286270494,
      "grad_norm": 5.6017560958862305,
      "learning_rate": 8.842105263157895e-06,
      "loss": 8.7513,
      "step": 16
    },
    {
      "epoch": 0.000302404673041624,
      "grad_norm": 6.31508731842041,
      "learning_rate": 8.736842105263158e-06,
      "loss": 9.7836,
      "step": 17
    },
    {
      "epoch": 0.00032019318322054304,
      "grad_norm": 6.612189292907715,
      "learning_rate": 8.631578947368422e-06,
      "loss": 9.8008,
      "step": 18
    },
    {
      "epoch": 0.00033798169339946214,
      "grad_norm": 6.52145528793335,
      "learning_rate": 8.526315789473685e-06,
      "loss": 9.7586,
      "step": 19
    },
    {
      "epoch": 0.0003557702035783812,
      "grad_norm": 3.751744508743286,
      "learning_rate": 8.421052631578948e-06,
      "loss": 6.328,
      "step": 20
    },
    {
      "epoch": 0.00037355871375730024,
      "grad_norm": 7.247509479522705,
      "learning_rate": 8.315789473684212e-06,
      "loss": 9.9537,
      "step": 21
    },
    {
      "epoch": 0.0003913472239362193,
      "grad_norm": 6.077712535858154,
      "learning_rate": 8.210526315789475e-06,
      "loss": 8.7607,
      "step": 22
    },
    {
      "epoch": 0.00040913573411513834,
      "grad_norm": 7.250381946563721,
      "learning_rate": 8.105263157894736e-06,
      "loss": 10.0158,
      "step": 23
    },
    {
      "epoch": 0.0004269242442940574,
      "grad_norm": 6.287909507751465,
      "learning_rate": 8.000000000000001e-06,
      "loss": 8.9885,
      "step": 24
    },
    {
      "epoch": 0.0004447127544729765,
      "grad_norm": 5.741156101226807,
      "learning_rate": 7.894736842105265e-06,
      "loss": 8.3258,
      "step": 25
    },
    {
      "epoch": 0.00046250126465189554,
      "grad_norm": 6.8032002449035645,
      "learning_rate": 7.789473684210526e-06,
      "loss": 9.4265,
      "step": 26
    },
    {
      "epoch": 0.0004802897748308146,
      "grad_norm": 6.104853630065918,
      "learning_rate": 7.68421052631579e-06,
      "loss": 8.6462,
      "step": 27
    },
    {
      "epoch": 0.0004980782850097337,
      "grad_norm": 5.974133014678955,
      "learning_rate": 7.578947368421054e-06,
      "loss": 8.5549,
      "step": 28
    },
    {
      "epoch": 0.0005158667951886527,
      "grad_norm": 6.735897064208984,
      "learning_rate": 7.473684210526316e-06,
      "loss": 8.8603,
      "step": 29
    },
    {
      "epoch": 0.0005336553053675718,
      "grad_norm": 7.271637439727783,
      "learning_rate": 7.368421052631579e-06,
      "loss": 9.3028,
      "step": 30
    },
    {
      "epoch": 0.0005514438155464908,
      "grad_norm": 4.994191646575928,
      "learning_rate": 7.263157894736843e-06,
      "loss": 7.2866,
      "step": 31
    },
    {
      "epoch": 0.0005692323257254099,
      "grad_norm": 6.951260089874268,
      "learning_rate": 7.157894736842106e-06,
      "loss": 8.9425,
      "step": 32
    },
    {
      "epoch": 0.000587020835904329,
      "grad_norm": 6.145110607147217,
      "learning_rate": 7.052631578947369e-06,
      "loss": 8.1539,
      "step": 33
    },
    {
      "epoch": 0.000604809346083248,
      "grad_norm": 6.2005934715271,
      "learning_rate": 6.947368421052632e-06,
      "loss": 8.2207,
      "step": 34
    },
    {
      "epoch": 0.0006225978562621671,
      "grad_norm": 7.999884128570557,
      "learning_rate": 6.842105263157896e-06,
      "loss": 9.2947,
      "step": 35
    },
    {
      "epoch": 0.0006403863664410861,
      "grad_norm": 5.5475993156433105,
      "learning_rate": 6.736842105263158e-06,
      "loss": 7.5838,
      "step": 36
    },
    {
      "epoch": 0.0006581748766200052,
      "grad_norm": 6.17701530456543,
      "learning_rate": 6.631578947368421e-06,
      "loss": 8.0581,
      "step": 37
    },
    {
      "epoch": 0.0006759633867989243,
      "grad_norm": 6.566706657409668,
      "learning_rate": 6.526315789473685e-06,
      "loss": 8.3187,
      "step": 38
    },
    {
      "epoch": 0.0006937518969778433,
      "grad_norm": 6.761594295501709,
      "learning_rate": 6.421052631578948e-06,
      "loss": 8.5625,
      "step": 39
    },
    {
      "epoch": 0.0007115404071567624,
      "grad_norm": 5.905500888824463,
      "learning_rate": 6.31578947368421e-06,
      "loss": 7.8564,
      "step": 40
    },
    {
      "epoch": 0.0007293289173356814,
      "grad_norm": 7.402847766876221,
      "learning_rate": 6.2105263157894745e-06,
      "loss": 8.484,
      "step": 41
    },
    {
      "epoch": 0.0007471174275146005,
      "grad_norm": 5.558272361755371,
      "learning_rate": 6.105263157894738e-06,
      "loss": 7.6174,
      "step": 42
    },
    {
      "epoch": 0.0007649059376935196,
      "grad_norm": 6.783658981323242,
      "learning_rate": 6e-06,
      "loss": 8.4544,
      "step": 43
    },
    {
      "epoch": 0.0007826944478724386,
      "grad_norm": 6.750698089599609,
      "learning_rate": 5.8947368421052634e-06,
      "loss": 8.5365,
      "step": 44
    },
    {
      "epoch": 0.0008004829580513577,
      "grad_norm": 7.054347038269043,
      "learning_rate": 5.789473684210527e-06,
      "loss": 8.5562,
      "step": 45
    },
    {
      "epoch": 0.0008182714682302767,
      "grad_norm": 7.174205303192139,
      "learning_rate": 5.68421052631579e-06,
      "loss": 8.3933,
      "step": 46
    },
    {
      "epoch": 0.0008360599784091958,
      "grad_norm": 6.223912715911865,
      "learning_rate": 5.578947368421052e-06,
      "loss": 7.6301,
      "step": 47
    },
    {
      "epoch": 0.0008538484885881148,
      "grad_norm": 6.72188663482666,
      "learning_rate": 5.4736842105263165e-06,
      "loss": 7.9856,
      "step": 48
    },
    {
      "epoch": 0.0008716369987670339,
      "grad_norm": 7.234642505645752,
      "learning_rate": 5.36842105263158e-06,
      "loss": 8.0845,
      "step": 49
    },
    {
      "epoch": 0.000889425508945953,
      "grad_norm": 6.163455963134766,
      "learning_rate": 5.263157894736842e-06,
      "loss": 7.9109,
      "step": 50
    },
    {
      "epoch": 0.000907214019124872,
      "grad_norm": 7.36458158493042,
      "learning_rate": 5.157894736842106e-06,
      "loss": 8.3442,
      "step": 51
    },
    {
      "epoch": 0.0009250025293037911,
      "grad_norm": 6.300682067871094,
      "learning_rate": 5.052631578947369e-06,
      "loss": 7.6125,
      "step": 52
    },
    {
      "epoch": 0.0009427910394827101,
      "grad_norm": 6.794142246246338,
      "learning_rate": 4.947368421052632e-06,
      "loss": 7.8155,
      "step": 53
    },
    {
      "epoch": 0.0009605795496616292,
      "grad_norm": 7.065849781036377,
      "learning_rate": 4.842105263157895e-06,
      "loss": 7.7995,
      "step": 54
    },
    {
      "epoch": 0.0009783680598405482,
      "grad_norm": 6.30277156829834,
      "learning_rate": 4.736842105263158e-06,
      "loss": 7.5334,
      "step": 55
    },
    {
      "epoch": 0.0009961565700194674,
      "grad_norm": 6.848322868347168,
      "learning_rate": 4.631578947368421e-06,
      "loss": 7.7645,
      "step": 56
    },
    {
      "epoch": 0.0010139450801983864,
      "grad_norm": 6.424248218536377,
      "learning_rate": 4.526315789473685e-06,
      "loss": 7.4624,
      "step": 57
    },
    {
      "epoch": 0.0010317335903773054,
      "grad_norm": 5.770240783691406,
      "learning_rate": 4.4210526315789476e-06,
      "loss": 7.1586,
      "step": 58
    },
    {
      "epoch": 0.0010495221005562244,
      "grad_norm": 6.985846042633057,
      "learning_rate": 4.315789473684211e-06,
      "loss": 8.0039,
      "step": 59
    },
    {
      "epoch": 0.0010673106107351436,
      "grad_norm": 5.9023051261901855,
      "learning_rate": 4.210526315789474e-06,
      "loss": 7.13,
      "step": 60
    },
    {
      "epoch": 0.0010850991209140626,
      "grad_norm": 6.839781284332275,
      "learning_rate": 4.105263157894737e-06,
      "loss": 7.7187,
      "step": 61
    },
    {
      "epoch": 0.0011028876310929816,
      "grad_norm": 5.6195292472839355,
      "learning_rate": 4.000000000000001e-06,
      "loss": 7.0968,
      "step": 62
    },
    {
      "epoch": 0.0011206761412719008,
      "grad_norm": 6.225754261016846,
      "learning_rate": 3.894736842105263e-06,
      "loss": 7.2991,
      "step": 63
    },
    {
      "epoch": 0.0011384646514508198,
      "grad_norm": 5.980795383453369,
      "learning_rate": 3.789473684210527e-06,
      "loss": 7.3844,
      "step": 64
    },
    {
      "epoch": 0.0011562531616297388,
      "grad_norm": 7.404062747955322,
      "learning_rate": 3.6842105263157896e-06,
      "loss": 7.9404,
      "step": 65
    },
    {
      "epoch": 0.001174041671808658,
      "grad_norm": 6.115976333618164,
      "learning_rate": 3.578947368421053e-06,
      "loss": 7.4668,
      "step": 66
    },
    {
      "epoch": 0.001191830181987577,
      "grad_norm": 5.997700214385986,
      "learning_rate": 3.473684210526316e-06,
      "loss": 7.4507,
      "step": 67
    },
    {
      "epoch": 0.001209618692166496,
      "grad_norm": 5.956457614898682,
      "learning_rate": 3.368421052631579e-06,
      "loss": 7.207,
      "step": 68
    },
    {
      "epoch": 0.001227407202345415,
      "grad_norm": 7.1224517822265625,
      "learning_rate": 3.2631578947368423e-06,
      "loss": 7.7513,
      "step": 69
    },
    {
      "epoch": 0.0012451957125243342,
      "grad_norm": 6.216053009033203,
      "learning_rate": 3.157894736842105e-06,
      "loss": 7.4178,
      "step": 70
    },
    {
      "epoch": 0.0012629842227032532,
      "grad_norm": 6.576999187469482,
      "learning_rate": 3.052631578947369e-06,
      "loss": 7.3703,
      "step": 71
    },
    {
      "epoch": 0.0012807727328821722,
      "grad_norm": 5.528324127197266,
      "learning_rate": 2.9473684210526317e-06,
      "loss": 6.8553,
      "step": 72
    },
    {
      "epoch": 0.0012985612430610914,
      "grad_norm": 5.759592533111572,
      "learning_rate": 2.842105263157895e-06,
      "loss": 7.0177,
      "step": 73
    },
    {
      "epoch": 0.0013163497532400104,
      "grad_norm": 6.481958866119385,
      "learning_rate": 2.7368421052631583e-06,
      "loss": 7.3719,
      "step": 74
    },
    {
      "epoch": 0.0013341382634189294,
      "grad_norm": 6.437648773193359,
      "learning_rate": 2.631578947368421e-06,
      "loss": 6.9652,
      "step": 75
    },
    {
      "epoch": 0.0013519267735978486,
      "grad_norm": 6.282752990722656,
      "learning_rate": 2.5263157894736844e-06,
      "loss": 7.0061,
      "step": 76
    },
    {
      "epoch": 0.0013697152837767676,
      "grad_norm": 6.8480095863342285,
      "learning_rate": 2.4210526315789477e-06,
      "loss": 6.9764,
      "step": 77
    },
    {
      "epoch": 0.0013875037939556866,
      "grad_norm": 7.648647785186768,
      "learning_rate": 2.3157894736842105e-06,
      "loss": 7.6782,
      "step": 78
    },
    {
      "epoch": 0.0014052923041346056,
      "grad_norm": 6.305149078369141,
      "learning_rate": 2.2105263157894738e-06,
      "loss": 7.2407,
      "step": 79
    },
    {
      "epoch": 0.0014230808143135248,
      "grad_norm": 4.783631801605225,
      "learning_rate": 2.105263157894737e-06,
      "loss": 6.1558,
      "step": 80
    },
    {
      "epoch": 0.0014408693244924438,
      "grad_norm": 5.74940824508667,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 7.1595,
      "step": 81
    },
    {
      "epoch": 0.0014586578346713628,
      "grad_norm": 5.990188121795654,
      "learning_rate": 1.8947368421052634e-06,
      "loss": 7.2115,
      "step": 82
    },
    {
      "epoch": 0.001476446344850282,
      "grad_norm": 5.920891284942627,
      "learning_rate": 1.7894736842105265e-06,
      "loss": 6.6953,
      "step": 83
    },
    {
      "epoch": 0.001494234855029201,
      "grad_norm": 6.066863536834717,
      "learning_rate": 1.6842105263157895e-06,
      "loss": 7.0951,
      "step": 84
    },
    {
      "epoch": 0.00151202336520812,
      "grad_norm": 5.827770709991455,
      "learning_rate": 1.5789473684210526e-06,
      "loss": 6.8988,
      "step": 85
    },
    {
      "epoch": 0.0015298118753870392,
      "grad_norm": 5.659218788146973,
      "learning_rate": 1.4736842105263159e-06,
      "loss": 6.8437,
      "step": 86
    },
    {
      "epoch": 0.0015476003855659582,
      "grad_norm": 5.702203750610352,
      "learning_rate": 1.3684210526315791e-06,
      "loss": 7.0453,
      "step": 87
    },
    {
      "epoch": 0.0015653888957448772,
      "grad_norm": 5.572003364562988,
      "learning_rate": 1.2631578947368422e-06,
      "loss": 6.8899,
      "step": 88
    },
    {
      "epoch": 0.0015831774059237962,
      "grad_norm": 5.5450544357299805,
      "learning_rate": 1.1578947368421053e-06,
      "loss": 6.8307,
      "step": 89
    },
    {
      "epoch": 0.0016009659161027154,
      "grad_norm": 5.988315105438232,
      "learning_rate": 1.0526315789473685e-06,
      "loss": 7.2495,
      "step": 90
    },
    {
      "epoch": 0.0016187544262816344,
      "grad_norm": 6.012745380401611,
      "learning_rate": 9.473684210526317e-07,
      "loss": 7.0681,
      "step": 91
    },
    {
      "epoch": 0.0016365429364605534,
      "grad_norm": 5.570369720458984,
      "learning_rate": 8.421052631578948e-07,
      "loss": 6.7353,
      "step": 92
    },
    {
      "epoch": 0.0016543314466394726,
      "grad_norm": 6.087926864624023,
      "learning_rate": 7.368421052631579e-07,
      "loss": 7.0446,
      "step": 93
    },
    {
      "epoch": 0.0016721199568183916,
      "grad_norm": 5.016287326812744,
      "learning_rate": 6.315789473684211e-07,
      "loss": 6.7244,
      "step": 94
    },
    {
      "epoch": 0.0016899084669973106,
      "grad_norm": 5.956068515777588,
      "learning_rate": 5.263157894736843e-07,
      "loss": 7.0284,
      "step": 95
    },
    {
      "epoch": 0.0017076969771762296,
      "grad_norm": 5.817139625549316,
      "learning_rate": 4.210526315789474e-07,
      "loss": 6.8571,
      "step": 96
    },
    {
      "epoch": 0.0017254854873551488,
      "grad_norm": 5.456579208374023,
      "learning_rate": 3.1578947368421055e-07,
      "loss": 6.8843,
      "step": 97
    },
    {
      "epoch": 0.0017432739975340678,
      "grad_norm": 5.519063949584961,
      "learning_rate": 2.105263157894737e-07,
      "loss": 6.7411,
      "step": 98
    },
    {
      "epoch": 0.0017610625077129868,
      "grad_norm": 6.552621841430664,
      "learning_rate": 1.0526315789473685e-07,
      "loss": 7.2883,
      "step": 99
    },
    {
      "epoch": 0.001778851017891906,
      "grad_norm": 5.3651347160339355,
      "learning_rate": 0.0,
      "loss": 6.8105,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 100,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 630481593272832.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
