{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0026682765268378587,
  "eval_steps": 500,
  "global_step": 150,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.778851017891906e-05,
      "grad_norm": 5.640249252319336,
      "learning_rate": 6.666666666666667e-07,
      "loss": 10.0472,
      "step": 1
    },
    {
      "epoch": 3.557702035783812e-05,
      "grad_norm": 5.3359575271606445,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 9.9103,
      "step": 2
    },
    {
      "epoch": 5.3365530536757174e-05,
      "grad_norm": 5.649725437164307,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 9.91,
      "step": 3
    },
    {
      "epoch": 7.115404071567624e-05,
      "grad_norm": 6.252277374267578,
      "learning_rate": 2.666666666666667e-06,
      "loss": 10.6934,
      "step": 4
    },
    {
      "epoch": 8.89425508945953e-05,
      "grad_norm": 4.859716892242432,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 9.1249,
      "step": 5
    },
    {
      "epoch": 0.00010673106107351435,
      "grad_norm": 6.081592082977295,
      "learning_rate": 4.000000000000001e-06,
      "loss": 10.6582,
      "step": 6
    },
    {
      "epoch": 0.00012451957125243342,
      "grad_norm": 6.438713073730469,
      "learning_rate": 4.666666666666667e-06,
      "loss": 10.8661,
      "step": 7
    },
    {
      "epoch": 0.00014230808143135247,
      "grad_norm": 5.9638190269470215,
      "learning_rate": 5.333333333333334e-06,
      "loss": 10.2628,
      "step": 8
    },
    {
      "epoch": 0.00016009659161027152,
      "grad_norm": 5.639041900634766,
      "learning_rate": 6e-06,
      "loss": 9.8225,
      "step": 9
    },
    {
      "epoch": 0.0001778851017891906,
      "grad_norm": 3.9086406230926514,
      "learning_rate": 6.666666666666667e-06,
      "loss": 7.4904,
      "step": 10
    },
    {
      "epoch": 0.00019567361196810965,
      "grad_norm": 6.750884532928467,
      "learning_rate": 7.333333333333333e-06,
      "loss": 11.0059,
      "step": 11
    },
    {
      "epoch": 0.0002134621221470287,
      "grad_norm": 6.078712463378906,
      "learning_rate": 8.000000000000001e-06,
      "loss": 10.2236,
      "step": 12
    },
    {
      "epoch": 0.00023125063232594777,
      "grad_norm": 6.4061970710754395,
      "learning_rate": 8.666666666666668e-06,
      "loss": 10.4073,
      "step": 13
    },
    {
      "epoch": 0.00024903914250486685,
      "grad_norm": 5.147252082824707,
      "learning_rate": 9.333333333333334e-06,
      "loss": 9.2323,
      "step": 14
    },
    {
      "epoch": 0.0002668276526837859,
      "grad_norm": 5.7242584228515625,
      "learning_rate": 1e-05,
      "loss": 9.8267,
      "step": 15
    },
    {
      "epoch": 0.00028461616286270494,
      "grad_norm": 5.489227771759033,
      "learning_rate": 9.925925925925927e-06,
      "loss": 8.9989,
      "step": 16
    },
    {
      "epoch": 0.000302404673041624,
      "grad_norm": 6.177982330322266,
      "learning_rate": 9.851851851851852e-06,
      "loss": 10.0606,
      "step": 17
    },
    {
      "epoch": 0.00032019318322054304,
      "grad_norm": 6.494633197784424,
      "learning_rate": 9.777777777777779e-06,
      "loss": 10.0819,
      "step": 18
    },
    {
      "epoch": 0.00033798169339946214,
      "grad_norm": 6.441848278045654,
      "learning_rate": 9.703703703703703e-06,
      "loss": 10.0367,
      "step": 19
    },
    {
      "epoch": 0.0003557702035783812,
      "grad_norm": 3.7021360397338867,
      "learning_rate": 9.62962962962963e-06,
      "loss": 6.4789,
      "step": 20
    },
    {
      "epoch": 0.00037355871375730024,
      "grad_norm": 7.147369861602783,
      "learning_rate": 9.555555555555556e-06,
      "loss": 10.2468,
      "step": 21
    },
    {
      "epoch": 0.0003913472239362193,
      "grad_norm": 6.021342754364014,
      "learning_rate": 9.481481481481483e-06,
      "loss": 8.9964,
      "step": 22
    },
    {
      "epoch": 0.00040913573411513834,
      "grad_norm": 7.185448169708252,
      "learning_rate": 9.407407407407408e-06,
      "loss": 10.2916,
      "step": 23
    },
    {
      "epoch": 0.0004269242442940574,
      "grad_norm": 6.228518962860107,
      "learning_rate": 9.333333333333334e-06,
      "loss": 9.2134,
      "step": 24
    },
    {
      "epoch": 0.0004447127544729765,
      "grad_norm": 5.725605010986328,
      "learning_rate": 9.25925925925926e-06,
      "loss": 8.5277,
      "step": 25
    },
    {
      "epoch": 0.00046250126465189554,
      "grad_norm": 6.788937568664551,
      "learning_rate": 9.185185185185186e-06,
      "loss": 9.6589,
      "step": 26
    },
    {
      "epoch": 0.0004802897748308146,
      "grad_norm": 6.104850769042969,
      "learning_rate": 9.111111111111112e-06,
      "loss": 8.8419,
      "step": 27
    },
    {
      "epoch": 0.0004980782850097337,
      "grad_norm": 5.9596099853515625,
      "learning_rate": 9.037037037037037e-06,
      "loss": 8.7347,
      "step": 28
    },
    {
      "epoch": 0.0005158667951886527,
      "grad_norm": 6.743227958679199,
      "learning_rate": 8.962962962962963e-06,
      "loss": 9.059,
      "step": 29
    },
    {
      "epoch": 0.0005336553053675718,
      "grad_norm": 7.28195858001709,
      "learning_rate": 8.888888888888888e-06,
      "loss": 9.5029,
      "step": 30
    },
    {
      "epoch": 0.0005514438155464908,
      "grad_norm": 5.00845193862915,
      "learning_rate": 8.814814814814817e-06,
      "loss": 7.4138,
      "step": 31
    },
    {
      "epoch": 0.0005692323257254099,
      "grad_norm": 6.990591526031494,
      "learning_rate": 8.740740740740741e-06,
      "loss": 9.1077,
      "step": 32
    },
    {
      "epoch": 0.000587020835904329,
      "grad_norm": 6.185837268829346,
      "learning_rate": 8.666666666666668e-06,
      "loss": 8.2882,
      "step": 33
    },
    {
      "epoch": 0.000604809346083248,
      "grad_norm": 6.250503063201904,
      "learning_rate": 8.592592592592593e-06,
      "loss": 8.3422,
      "step": 34
    },
    {
      "epoch": 0.0006225978562621671,
      "grad_norm": 8.091031074523926,
      "learning_rate": 8.518518518518519e-06,
      "loss": 9.4368,
      "step": 35
    },
    {
      "epoch": 0.0006403863664410861,
      "grad_norm": 5.61014461517334,
      "learning_rate": 8.444444444444446e-06,
      "loss": 7.6709,
      "step": 36
    },
    {
      "epoch": 0.0006581748766200052,
      "grad_norm": NaN,
      "learning_rate": 8.444444444444446e-06,
      "loss": 12.5676,
      "step": 37
    },
    {
      "epoch": 0.0006759633867989243,
      "grad_norm": 6.642368793487549,
      "learning_rate": 8.37037037037037e-06,
      "loss": 8.4588,
      "step": 38
    },
    {
      "epoch": 0.0006937518969778433,
      "grad_norm": 6.8221330642700195,
      "learning_rate": 8.296296296296297e-06,
      "loss": 8.6901,
      "step": 39
    },
    {
      "epoch": 0.0007115404071567624,
      "grad_norm": 5.975425720214844,
      "learning_rate": 8.222222222222222e-06,
      "loss": 7.9532,
      "step": 40
    },
    {
      "epoch": 0.0007293289173356814,
      "grad_norm": 7.5021562576293945,
      "learning_rate": 8.148148148148148e-06,
      "loss": 8.5919,
      "step": 41
    },
    {
      "epoch": 0.0007471174275146005,
      "grad_norm": 5.613699913024902,
      "learning_rate": 8.074074074074075e-06,
      "loss": 7.6821,
      "step": 42
    },
    {
      "epoch": 0.0007649059376935196,
      "grad_norm": 6.851784706115723,
      "learning_rate": 8.000000000000001e-06,
      "loss": 8.5184,
      "step": 43
    },
    {
      "epoch": 0.0007826944478724386,
      "grad_norm": 6.816142559051514,
      "learning_rate": 7.925925925925926e-06,
      "loss": 8.5835,
      "step": 44
    },
    {
      "epoch": 0.0008004829580513577,
      "grad_norm": 7.114953517913818,
      "learning_rate": 7.851851851851853e-06,
      "loss": 8.5867,
      "step": 45
    },
    {
      "epoch": 0.0008182714682302767,
      "grad_norm": 7.23154878616333,
      "learning_rate": 7.77777777777778e-06,
      "loss": 8.4051,
      "step": 46
    },
    {
      "epoch": 0.0008360599784091958,
      "grad_norm": 6.267953872680664,
      "learning_rate": 7.703703703703704e-06,
      "loss": 7.625,
      "step": 47
    },
    {
      "epoch": 0.0008538484885881148,
      "grad_norm": 6.764136791229248,
      "learning_rate": 7.62962962962963e-06,
      "loss": 7.9621,
      "step": 48
    },
    {
      "epoch": 0.0008716369987670339,
      "grad_norm": 7.266951560974121,
      "learning_rate": 7.555555555555556e-06,
      "loss": 8.0419,
      "step": 49
    },
    {
      "epoch": 0.000889425508945953,
      "grad_norm": 6.200250625610352,
      "learning_rate": 7.481481481481482e-06,
      "loss": 7.8596,
      "step": 50
    },
    {
      "epoch": 0.000907214019124872,
      "grad_norm": 7.388625621795654,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 8.2619,
      "step": 51
    },
    {
      "epoch": 0.0009250025293037911,
      "grad_norm": 6.277222156524658,
      "learning_rate": 7.333333333333333e-06,
      "loss": 7.5227,
      "step": 52
    },
    {
      "epoch": 0.0009427910394827101,
      "grad_norm": 6.761936664581299,
      "learning_rate": 7.2592592592592605e-06,
      "loss": 7.7018,
      "step": 53
    },
    {
      "epoch": 0.0009605795496616292,
      "grad_norm": 7.020855903625488,
      "learning_rate": 7.185185185185186e-06,
      "loss": 7.6614,
      "step": 54
    },
    {
      "epoch": 0.0009783680598405482,
      "grad_norm": 6.2798614501953125,
      "learning_rate": 7.111111111111112e-06,
      "loss": 7.3927,
      "step": 55
    },
    {
      "epoch": 0.0009961565700194674,
      "grad_norm": 6.7986226081848145,
      "learning_rate": 7.0370370370370375e-06,
      "loss": 7.5971,
      "step": 56
    },
    {
      "epoch": 0.0010139450801983864,
      "grad_norm": 6.312342166900635,
      "learning_rate": 6.962962962962964e-06,
      "loss": 7.2777,
      "step": 57
    },
    {
      "epoch": 0.0010317335903773054,
      "grad_norm": 5.679391860961914,
      "learning_rate": 6.88888888888889e-06,
      "loss": 6.9799,
      "step": 58
    },
    {
      "epoch": 0.0010495221005562244,
      "grad_norm": 6.8848981857299805,
      "learning_rate": 6.814814814814815e-06,
      "loss": 7.7762,
      "step": 59
    },
    {
      "epoch": 0.0010673106107351436,
      "grad_norm": 5.782047748565674,
      "learning_rate": 6.740740740740741e-06,
      "loss": 6.9183,
      "step": 60
    },
    {
      "epoch": 0.0010850991209140626,
      "grad_norm": 6.658273220062256,
      "learning_rate": 6.666666666666667e-06,
      "loss": 7.4531,
      "step": 61
    },
    {
      "epoch": 0.0011028876310929816,
      "grad_norm": 5.420024394989014,
      "learning_rate": 6.592592592592592e-06,
      "loss": 6.8659,
      "step": 62
    },
    {
      "epoch": 0.0011206761412719008,
      "grad_norm": 6.001608848571777,
      "learning_rate": 6.51851851851852e-06,
      "loss": 7.024,
      "step": 63
    },
    {
      "epoch": 0.0011384646514508198,
      "grad_norm": 5.684037208557129,
      "learning_rate": 6.444444444444445e-06,
      "loss": 7.1002,
      "step": 64
    },
    {
      "epoch": 0.0011562531616297388,
      "grad_norm": 7.070574760437012,
      "learning_rate": 6.370370370370371e-06,
      "loss": 7.556,
      "step": 65
    },
    {
      "epoch": 0.001174041671808658,
      "grad_norm": 5.825118064880371,
      "learning_rate": 6.296296296296297e-06,
      "loss": 7.1539,
      "step": 66
    },
    {
      "epoch": 0.001191830181987577,
      "grad_norm": 5.640423774719238,
      "learning_rate": 6.222222222222223e-06,
      "loss": 7.1168,
      "step": 67
    },
    {
      "epoch": 0.001209618692166496,
      "grad_norm": 5.52203369140625,
      "learning_rate": 6.148148148148149e-06,
      "loss": 6.8565,
      "step": 68
    },
    {
      "epoch": 0.001227407202345415,
      "grad_norm": 6.662032127380371,
      "learning_rate": 6.0740740740740745e-06,
      "loss": 7.3187,
      "step": 69
    },
    {
      "epoch": 0.0012451957125243342,
      "grad_norm": 5.759033203125,
      "learning_rate": 6e-06,
      "loss": 7.0256,
      "step": 70
    },
    {
      "epoch": 0.0012629842227032532,
      "grad_norm": 5.89305305480957,
      "learning_rate": 5.925925925925926e-06,
      "loss": 6.93,
      "step": 71
    },
    {
      "epoch": 0.0012807727328821722,
      "grad_norm": 4.813185214996338,
      "learning_rate": 5.8518518518518515e-06,
      "loss": 6.4715,
      "step": 72
    },
    {
      "epoch": 0.0012985612430610914,
      "grad_norm": 4.945833683013916,
      "learning_rate": 5.777777777777778e-06,
      "loss": 6.616,
      "step": 73
    },
    {
      "epoch": 0.0013163497532400104,
      "grad_norm": 5.671093463897705,
      "learning_rate": 5.7037037037037045e-06,
      "loss": 6.8983,
      "step": 74
    },
    {
      "epoch": 0.0013341382634189294,
      "grad_norm": 5.342313289642334,
      "learning_rate": 5.62962962962963e-06,
      "loss": 6.4838,
      "step": 75
    },
    {
      "epoch": 0.0013519267735978486,
      "grad_norm": 5.441424369812012,
      "learning_rate": 5.555555555555557e-06,
      "loss": 6.4998,
      "step": 76
    },
    {
      "epoch": 0.0013697152837767676,
      "grad_norm": 5.718043327331543,
      "learning_rate": 5.481481481481482e-06,
      "loss": 6.4255,
      "step": 77
    },
    {
      "epoch": 0.0013875037939556866,
      "grad_norm": 6.341522693634033,
      "learning_rate": 5.407407407407408e-06,
      "loss": 7.0655,
      "step": 78
    },
    {
      "epoch": 0.0014052923041346056,
      "grad_norm": 5.257484436035156,
      "learning_rate": 5.333333333333334e-06,
      "loss": 6.7047,
      "step": 79
    },
    {
      "epoch": 0.0014230808143135248,
      "grad_norm": 3.9998867511749268,
      "learning_rate": 5.259259259259259e-06,
      "loss": 5.7353,
      "step": 80
    },
    {
      "epoch": 0.0014408693244924438,
      "grad_norm": 4.638749122619629,
      "learning_rate": 5.185185185185185e-06,
      "loss": 6.6304,
      "step": 81
    },
    {
      "epoch": 0.0014586578346713628,
      "grad_norm": 4.8028883934021,
      "learning_rate": 5.1111111111111115e-06,
      "loss": 6.6467,
      "step": 82
    },
    {
      "epoch": 0.001476446344850282,
      "grad_norm": 4.700263977050781,
      "learning_rate": 5.037037037037037e-06,
      "loss": 6.1439,
      "step": 83
    },
    {
      "epoch": 0.001494234855029201,
      "grad_norm": 4.846108436584473,
      "learning_rate": 4.962962962962964e-06,
      "loss": 6.4985,
      "step": 84
    },
    {
      "epoch": 0.00151202336520812,
      "grad_norm": 4.647286415100098,
      "learning_rate": 4.888888888888889e-06,
      "loss": 6.2954,
      "step": 85
    },
    {
      "epoch": 0.0015298118753870392,
      "grad_norm": 4.407411098480225,
      "learning_rate": 4.814814814814815e-06,
      "loss": 6.2902,
      "step": 86
    },
    {
      "epoch": 0.0015476003855659582,
      "grad_norm": 4.418837070465088,
      "learning_rate": 4.7407407407407415e-06,
      "loss": 6.4288,
      "step": 87
    },
    {
      "epoch": 0.0015653888957448772,
      "grad_norm": 4.373396396636963,
      "learning_rate": 4.666666666666667e-06,
      "loss": 6.2996,
      "step": 88
    },
    {
      "epoch": 0.0015831774059237962,
      "grad_norm": 4.256215572357178,
      "learning_rate": 4.592592592592593e-06,
      "loss": 6.2157,
      "step": 89
    },
    {
      "epoch": 0.0016009659161027154,
      "grad_norm": 4.704657077789307,
      "learning_rate": 4.5185185185185185e-06,
      "loss": 6.5909,
      "step": 90
    },
    {
      "epoch": 0.0016187544262816344,
      "grad_norm": 4.423285484313965,
      "learning_rate": 4.444444444444444e-06,
      "loss": 6.3897,
      "step": 91
    },
    {
      "epoch": 0.0016365429364605534,
      "grad_norm": 4.194725036621094,
      "learning_rate": 4.370370370370371e-06,
      "loss": 6.0956,
      "step": 92
    },
    {
      "epoch": 0.0016543314466394726,
      "grad_norm": 4.5506205558776855,
      "learning_rate": 4.296296296296296e-06,
      "loss": 6.2963,
      "step": 93
    },
    {
      "epoch": 0.0016721199568183916,
      "grad_norm": 3.7785236835479736,
      "learning_rate": 4.222222222222223e-06,
      "loss": 6.1138,
      "step": 94
    },
    {
      "epoch": 0.0016899084669973106,
      "grad_norm": 4.52910041809082,
      "learning_rate": 4.1481481481481485e-06,
      "loss": 6.2944,
      "step": 95
    },
    {
      "epoch": 0.0017076969771762296,
      "grad_norm": 4.410584926605225,
      "learning_rate": 4.074074074074074e-06,
      "loss": 6.1162,
      "step": 96
    },
    {
      "epoch": 0.0017254854873551488,
      "grad_norm": 4.011416912078857,
      "learning_rate": 4.000000000000001e-06,
      "loss": 6.1862,
      "step": 97
    },
    {
      "epoch": 0.0017432739975340678,
      "grad_norm": 4.050644397735596,
      "learning_rate": 3.925925925925926e-06,
      "loss": 6.0052,
      "step": 98
    },
    {
      "epoch": 0.0017610625077129868,
      "grad_norm": 4.878848552703857,
      "learning_rate": 3.851851851851852e-06,
      "loss": 6.4206,
      "step": 99
    },
    {
      "epoch": 0.001778851017891906,
      "grad_norm": 4.036311149597168,
      "learning_rate": 3.777777777777778e-06,
      "loss": 6.0681,
      "step": 100
    },
    {
      "epoch": 0.001796639528070825,
      "grad_norm": 3.8263726234436035,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 6.0232,
      "step": 101
    },
    {
      "epoch": 0.001814428038249744,
      "grad_norm": 4.464507102966309,
      "learning_rate": 3.6296296296296302e-06,
      "loss": 6.1646,
      "step": 102
    },
    {
      "epoch": 0.0018322165484286632,
      "grad_norm": 3.8286845684051514,
      "learning_rate": 3.555555555555556e-06,
      "loss": 6.1284,
      "step": 103
    },
    {
      "epoch": 0.0018500050586075822,
      "grad_norm": 3.978696823120117,
      "learning_rate": 3.481481481481482e-06,
      "loss": 6.1011,
      "step": 104
    },
    {
      "epoch": 0.0018677935687865012,
      "grad_norm": 3.471497058868408,
      "learning_rate": 3.4074074074074077e-06,
      "loss": 6.0629,
      "step": 105
    },
    {
      "epoch": 0.0018855820789654202,
      "grad_norm": 3.780287742614746,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 6.1376,
      "step": 106
    },
    {
      "epoch": 0.0019033705891443394,
      "grad_norm": 3.6561033725738525,
      "learning_rate": 3.25925925925926e-06,
      "loss": 6.1086,
      "step": 107
    },
    {
      "epoch": 0.0019211590993232584,
      "grad_norm": 3.619263172149658,
      "learning_rate": 3.1851851851851855e-06,
      "loss": 5.7888,
      "step": 108
    },
    {
      "epoch": 0.0019389476095021774,
      "grad_norm": 3.8255085945129395,
      "learning_rate": 3.1111111111111116e-06,
      "loss": 6.0841,
      "step": 109
    },
    {
      "epoch": 0.0019567361196810963,
      "grad_norm": 4.0171732902526855,
      "learning_rate": 3.0370370370370372e-06,
      "loss": 6.1441,
      "step": 110
    },
    {
      "epoch": 0.0019745246298600153,
      "grad_norm": 3.8204185962677,
      "learning_rate": 2.962962962962963e-06,
      "loss": 6.0358,
      "step": 111
    },
    {
      "epoch": 0.0019923131400389348,
      "grad_norm": 2.985891580581665,
      "learning_rate": 2.888888888888889e-06,
      "loss": 5.0876,
      "step": 112
    },
    {
      "epoch": 0.0020101016502178538,
      "grad_norm": 4.050252437591553,
      "learning_rate": 2.814814814814815e-06,
      "loss": 6.3169,
      "step": 113
    },
    {
      "epoch": 0.0020278901603967728,
      "grad_norm": 3.5812759399414062,
      "learning_rate": 2.740740740740741e-06,
      "loss": 6.0414,
      "step": 114
    },
    {
      "epoch": 0.0020456786705756918,
      "grad_norm": 3.775737762451172,
      "learning_rate": 2.666666666666667e-06,
      "loss": 5.717,
      "step": 115
    },
    {
      "epoch": 0.0020634671807546108,
      "grad_norm": 3.6585705280303955,
      "learning_rate": 2.5925925925925925e-06,
      "loss": 5.7405,
      "step": 116
    },
    {
      "epoch": 0.0020812556909335297,
      "grad_norm": 3.8140106201171875,
      "learning_rate": 2.5185185185185186e-06,
      "loss": 5.8802,
      "step": 117
    },
    {
      "epoch": 0.0020990442011124487,
      "grad_norm": 3.5611367225646973,
      "learning_rate": 2.4444444444444447e-06,
      "loss": 5.7119,
      "step": 118
    },
    {
      "epoch": 0.002116832711291368,
      "grad_norm": 3.3441193103790283,
      "learning_rate": 2.3703703703703707e-06,
      "loss": 5.7489,
      "step": 119
    },
    {
      "epoch": 0.002134621221470287,
      "grad_norm": 3.6451096534729004,
      "learning_rate": 2.2962962962962964e-06,
      "loss": 5.7368,
      "step": 120
    },
    {
      "epoch": 0.002152409731649206,
      "grad_norm": 3.5076589584350586,
      "learning_rate": 2.222222222222222e-06,
      "loss": 5.9407,
      "step": 121
    },
    {
      "epoch": 0.002170198241828125,
      "grad_norm": 3.538865804672241,
      "learning_rate": 2.148148148148148e-06,
      "loss": 5.8608,
      "step": 122
    },
    {
      "epoch": 0.002187986752007044,
      "grad_norm": 3.9734456539154053,
      "learning_rate": 2.0740740740740742e-06,
      "loss": 6.0291,
      "step": 123
    },
    {
      "epoch": 0.002205775262185963,
      "grad_norm": 4.064731597900391,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 6.0368,
      "step": 124
    },
    {
      "epoch": 0.0022235637723648826,
      "grad_norm": 4.28141975402832,
      "learning_rate": 1.925925925925926e-06,
      "loss": 5.9829,
      "step": 125
    },
    {
      "epoch": 0.0022413522825438016,
      "grad_norm": 3.544639825820923,
      "learning_rate": 1.8518518518518519e-06,
      "loss": 5.9572,
      "step": 126
    },
    {
      "epoch": 0.0022591407927227206,
      "grad_norm": 3.8584654331207275,
      "learning_rate": 1.777777777777778e-06,
      "loss": 6.0588,
      "step": 127
    },
    {
      "epoch": 0.0022769293029016396,
      "grad_norm": 4.332820892333984,
      "learning_rate": 1.7037037037037038e-06,
      "loss": 5.947,
      "step": 128
    },
    {
      "epoch": 0.0022947178130805585,
      "grad_norm": 3.6252541542053223,
      "learning_rate": 1.62962962962963e-06,
      "loss": 5.8705,
      "step": 129
    },
    {
      "epoch": 0.0023125063232594775,
      "grad_norm": 2.829103469848633,
      "learning_rate": 1.5555555555555558e-06,
      "loss": 5.5231,
      "step": 130
    },
    {
      "epoch": 0.0023302948334383965,
      "grad_norm": 3.8421010971069336,
      "learning_rate": 1.4814814814814815e-06,
      "loss": 5.8803,
      "step": 131
    },
    {
      "epoch": 0.002348083343617316,
      "grad_norm": 3.905120372772217,
      "learning_rate": 1.4074074074074075e-06,
      "loss": 5.5639,
      "step": 132
    },
    {
      "epoch": 0.002365871853796235,
      "grad_norm": 2.924635648727417,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 5.4512,
      "step": 133
    },
    {
      "epoch": 0.002383660363975154,
      "grad_norm": 4.225833892822266,
      "learning_rate": 1.2592592592592593e-06,
      "loss": 5.6486,
      "step": 134
    },
    {
      "epoch": 0.002401448874154073,
      "grad_norm": 3.1741812229156494,
      "learning_rate": 1.1851851851851854e-06,
      "loss": 5.6028,
      "step": 135
    },
    {
      "epoch": 0.002419237384332992,
      "grad_norm": 2.706515312194824,
      "learning_rate": 1.111111111111111e-06,
      "loss": 5.348,
      "step": 136
    },
    {
      "epoch": 0.002437025894511911,
      "grad_norm": 3.2014267444610596,
      "learning_rate": 1.0370370370370371e-06,
      "loss": 5.9326,
      "step": 137
    },
    {
      "epoch": 0.00245481440469083,
      "grad_norm": 3.8718209266662598,
      "learning_rate": 9.62962962962963e-07,
      "loss": 5.6877,
      "step": 138
    },
    {
      "epoch": 0.0024726029148697494,
      "grad_norm": 3.7178237438201904,
      "learning_rate": 8.88888888888889e-07,
      "loss": 5.4477,
      "step": 139
    },
    {
      "epoch": 0.0024903914250486684,
      "grad_norm": 3.86826229095459,
      "learning_rate": 8.14814814814815e-07,
      "loss": 5.7587,
      "step": 140
    },
    {
      "epoch": 0.0025081799352275874,
      "grad_norm": 3.3556106090545654,
      "learning_rate": 7.407407407407407e-07,
      "loss": 5.9845,
      "step": 141
    },
    {
      "epoch": 0.0025259684454065063,
      "grad_norm": 3.1089587211608887,
      "learning_rate": 6.666666666666667e-07,
      "loss": 5.593,
      "step": 142
    },
    {
      "epoch": 0.0025437569555854253,
      "grad_norm": 3.935248613357544,
      "learning_rate": 5.925925925925927e-07,
      "loss": 5.9652,
      "step": 143
    },
    {
      "epoch": 0.0025615454657643443,
      "grad_norm": 2.707876205444336,
      "learning_rate": 5.185185185185186e-07,
      "loss": 5.6866,
      "step": 144
    },
    {
      "epoch": 0.0025793339759432638,
      "grad_norm": 3.879394292831421,
      "learning_rate": 4.444444444444445e-07,
      "loss": 6.0409,
      "step": 145
    },
    {
      "epoch": 0.0025971224861221828,
      "grad_norm": 4.214433670043945,
      "learning_rate": 3.7037037037037036e-07,
      "loss": 5.8986,
      "step": 146
    },
    {
      "epoch": 0.0026149109963011018,
      "grad_norm": 3.366825819015503,
      "learning_rate": 2.9629629629629634e-07,
      "loss": 5.8893,
      "step": 147
    },
    {
      "epoch": 0.0026326995064800207,
      "grad_norm": 3.4596517086029053,
      "learning_rate": 2.2222222222222224e-07,
      "loss": 5.6779,
      "step": 148
    },
    {
      "epoch": 0.0026504880166589397,
      "grad_norm": 3.619464159011841,
      "learning_rate": 1.4814814814814817e-07,
      "loss": 5.7266,
      "step": 149
    },
    {
      "epoch": 0.0026682765268378587,
      "grad_norm": 3.2892532348632812,
      "learning_rate": 7.407407407407409e-08,
      "loss": 5.8745,
      "step": 150
    }
  ],
  "logging_steps": 1,
  "max_steps": 150,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 150,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 944263365714432.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
