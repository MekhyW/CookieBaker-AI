{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmekhyw\u001b[0m (\u001b[33mmekhyw-insper\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\felip\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b44e6cc70c3483684bcff1fb2f674f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = \"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_name = \"IlyaGusev/gemma-2-2b-it-abliterated\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    def format_chat(example):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example['query']},\n",
    "            {\"role\": \"assistant\", \"content\": example['response']}\n",
    "        ]\n",
    "        formatted_chat = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        return {\"text\": formatted_chat}\n",
    "    formatted_dataset = dataset.map(format_chat)\n",
    "    formatted_dataset = formatted_dataset['train'].remove_columns(\n",
    "        [col for col in formatted_dataset['train'].column_names if col != \"text\"]\n",
    "    )\n",
    "    return formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2090473\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sfw = datasets.load_dataset(\"parquet\", data_files=\"../data/SFW_qa.parquet\")\n",
    "dataset_sfw = dataset_sfw.shuffle(seed=42)\n",
    "dataset_sfw = prepare_dataset(dataset_sfw)\n",
    "dataset_sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb06e27f2bd54c41b6d2e0e5ccc5d805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21668668494f4b529e3b563126bff7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/899457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 899457\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nsfw = datasets.load_dataset(\"parquet\", data_files=\"../data/NSFW_qa.parquet\")\n",
    "dataset_nsfw = dataset_nsfw.shuffle(seed=42)\n",
    "dataset_nsfw = prepare_dataset(dataset_nsfw)\n",
    "dataset_nsfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer_sfw = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_sfw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=15,\n",
    "        max_steps=150,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        output_dir=\"../models/SFW\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\CookieBaker-AI\\training\\notebooks\\wandb\\run-20241129_020131-gp12ento</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento' target=\"_blank\">dutiful-frog-16</a></strong> to <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17091d698b874261bb58e0557f436fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.303, 'grad_norm': 4.914122581481934, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.0}\n",
      "{'loss': 8.2915, 'grad_norm': 4.106173515319824, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5451, 'grad_norm': 4.586526393890381, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3551, 'grad_norm': 4.605806827545166, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 8.44, 'grad_norm': 4.779813766479492, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 9.235, 'grad_norm': 5.068270683288574, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3515, 'grad_norm': 4.300962924957275, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 10.4071, 'grad_norm': 6.471405982971191, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 8.834, 'grad_norm': 4.997831344604492, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5594, 'grad_norm': 5.541932582855225, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4003, 'grad_norm': 5.3805251121521, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2469, 'grad_norm': 5.334012508392334, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3799, 'grad_norm': 5.680000305175781, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0755, 'grad_norm': 6.499693870544434, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.7534, 'grad_norm': 5.807558059692383, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 8.6168, 'grad_norm': 5.530555725097656, 'learning_rate': 9.925925925925927e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6854, 'grad_norm': 3.17997145652771, 'learning_rate': 9.851851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6531, 'grad_norm': 5.3292975425720215, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2118, 'grad_norm': 4.616573810577393, 'learning_rate': 9.703703703703703e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3419, 'grad_norm': 4.261197090148926, 'learning_rate': 9.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1244, 'grad_norm': 5.016082286834717, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3252, 'grad_norm': 6.5429368019104, 'learning_rate': 9.481481481481483e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8916, 'grad_norm': 4.747773170471191, 'learning_rate': 9.407407407407408e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5566, 'grad_norm': 3.7492105960845947, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4832, 'grad_norm': 6.656974792480469, 'learning_rate': 9.25925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1505, 'grad_norm': 5.176398277282715, 'learning_rate': 9.185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2091, 'grad_norm': 6.8035078048706055, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5524, 'grad_norm': 5.163696765899658, 'learning_rate': 9.037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 7.842, 'grad_norm': 5.270222187042236, 'learning_rate': 8.962962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6657, 'grad_norm': 5.733499050140381, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8987, 'grad_norm': 5.23502779006958, 'learning_rate': 8.814814814814817e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1228, 'grad_norm': 4.630730628967285, 'learning_rate': 8.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3445, 'grad_norm': 4.345437526702881, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}\n",
      "{'loss': 7.692, 'grad_norm': 5.195389270782471, 'learning_rate': 8.592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2675, 'grad_norm': 6.0369648933410645, 'learning_rate': 8.518518518518519e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0948, 'grad_norm': 5.991506576538086, 'learning_rate': 8.444444444444446e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0787, 'grad_norm': 5.727706432342529, 'learning_rate': 8.37037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6768, 'grad_norm': 6.803436279296875, 'learning_rate': 8.296296296296297e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7225, 'grad_norm': 5.808382987976074, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5067, 'grad_norm': 5.463085651397705, 'learning_rate': 8.148148148148148e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9161, 'grad_norm': 5.968760967254639, 'learning_rate': 8.074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1011, 'grad_norm': nan, 'learning_rate': 8.074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1256, 'grad_norm': 5.110297203063965, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3246, 'grad_norm': 5.460613250732422, 'learning_rate': 7.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8548, 'grad_norm': 5.898072242736816, 'learning_rate': 7.851851851851853e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3731, 'grad_norm': 5.975387096405029, 'learning_rate': 7.77777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0464, 'grad_norm': 5.117428302764893, 'learning_rate': 7.703703703703704e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3814, 'grad_norm': 5.755771636962891, 'learning_rate': 7.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9231, 'grad_norm': 6.621214389801025, 'learning_rate': 7.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1117, 'grad_norm': 5.408539772033691, 'learning_rate': 7.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1625, 'grad_norm': 5.642579555511475, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2217, 'grad_norm': 6.34633207321167, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 7.394, 'grad_norm': 5.812069892883301, 'learning_rate': 7.2592592592592605e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4181, 'grad_norm': nan, 'learning_rate': 7.2592592592592605e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0971, 'grad_norm': 5.618387222290039, 'learning_rate': 7.185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 7.068, 'grad_norm': 5.955078601837158, 'learning_rate': 7.111111111111112e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3285, 'grad_norm': 6.2190752029418945, 'learning_rate': 7.0370370370370375e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4479, 'grad_norm': 6.063862323760986, 'learning_rate': 6.962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7654, 'grad_norm': 4.723182678222656, 'learning_rate': 6.88888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5648, 'grad_norm': 4.796474456787109, 'learning_rate': 6.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6181, 'grad_norm': 5.125892162322998, 'learning_rate': 6.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6356, 'grad_norm': 5.393930912017822, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2953, 'grad_norm': 4.747994422912598, 'learning_rate': 6.592592592592592e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1058, 'grad_norm': 5.898232936859131, 'learning_rate': 6.51851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8395, 'grad_norm': 5.573493003845215, 'learning_rate': 6.444444444444445e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7017, 'grad_norm': 5.055373668670654, 'learning_rate': 6.370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8297, 'grad_norm': 5.677459716796875, 'learning_rate': 6.296296296296297e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4863, 'grad_norm': 5.001908302307129, 'learning_rate': 6.222222222222223e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9306, 'grad_norm': 5.986841201782227, 'learning_rate': 6.148148148148149e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9721, 'grad_norm': 4.351851940155029, 'learning_rate': 6.0740740740740745e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5415, 'grad_norm': 5.399590492248535, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0908, 'grad_norm': 4.37050199508667, 'learning_rate': 5.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5465, 'grad_norm': 5.043421745300293, 'learning_rate': 5.8518518518518515e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4116, 'grad_norm': 5.020814895629883, 'learning_rate': 5.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 6.29, 'grad_norm': 4.505062103271484, 'learning_rate': 5.7037037037037045e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5421, 'grad_norm': 5.0331292152404785, 'learning_rate': 5.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0529, 'grad_norm': 4.157555103302002, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6469, 'grad_norm': 5.222238540649414, 'learning_rate': 5.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5831, 'grad_norm': 4.665149688720703, 'learning_rate': 5.407407407407408e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5081, 'grad_norm': 5.100517272949219, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2121, 'grad_norm': 3.765774726867676, 'learning_rate': 5.259259259259259e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8025, 'grad_norm': 3.6263539791107178, 'learning_rate': 5.185185185185185e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4114, 'grad_norm': 4.124402046203613, 'learning_rate': 5.1111111111111115e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9249, 'grad_norm': 3.9898059368133545, 'learning_rate': 5.037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7508, 'grad_norm': 2.743068218231201, 'learning_rate': 4.962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8384, 'grad_norm': 3.7696239948272705, 'learning_rate': 4.888888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9986, 'grad_norm': 4.254659175872803, 'learning_rate': 4.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4962, 'grad_norm': 3.334810733795166, 'learning_rate': 4.7407407407407415e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9401, 'grad_norm': 3.990863084793091, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3713, 'grad_norm': 3.4022581577301025, 'learning_rate': 4.592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6035, 'grad_norm': 3.8497114181518555, 'learning_rate': 4.5185185185185185e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8795, 'grad_norm': 3.5570859909057617, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.0}\n",
      "{'loss': 5.93, 'grad_norm': 4.035972595214844, 'learning_rate': 4.370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0516, 'grad_norm': 4.05921745300293, 'learning_rate': 4.296296296296296e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9764, 'grad_norm': 3.772160768508911, 'learning_rate': 4.222222222222223e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9738, 'grad_norm': 3.3748414516448975, 'learning_rate': 4.1481481481481485e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2136, 'grad_norm': 3.999753475189209, 'learning_rate': 4.074074074074074e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4391, 'grad_norm': 2.992340564727783, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7702, 'grad_norm': 3.478863477706909, 'learning_rate': 3.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0847, 'grad_norm': 3.7847518920898438, 'learning_rate': 3.851851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9924, 'grad_norm': 3.8556759357452393, 'learning_rate': 3.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8544, 'grad_norm': 3.3205361366271973, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1714, 'grad_norm': 3.887319803237915, 'learning_rate': 3.6296296296296302e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7103, 'grad_norm': 3.2147443294525146, 'learning_rate': 3.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5731, 'grad_norm': 2.7577788829803467, 'learning_rate': 3.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 5.898, 'grad_norm': 4.1066813468933105, 'learning_rate': 3.4074074074074077e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0247, 'grad_norm': 3.871016263961792, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0922, 'grad_norm': 4.025548934936523, 'learning_rate': 3.25925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8186, 'grad_norm': 3.630462169647217, 'learning_rate': 3.1851851851851855e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0076, 'grad_norm': 3.6633527278900146, 'learning_rate': 3.1111111111111116e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8289, 'grad_norm': 3.505941152572632, 'learning_rate': 3.0370370370370372e-06, 'epoch': 0.0}\n",
      "{'loss': 5.302, 'grad_norm': 3.1208062171936035, 'learning_rate': 2.962962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6574, 'grad_norm': 3.237488269805908, 'learning_rate': 2.888888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3308, 'grad_norm': 3.0538713932037354, 'learning_rate': 2.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8899, 'grad_norm': 2.5305652618408203, 'learning_rate': 2.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4036, 'grad_norm': 2.9985098838806152, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8347, 'grad_norm': 3.740753173828125, 'learning_rate': 2.5925925925925925e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9302, 'grad_norm': 3.9635252952575684, 'learning_rate': 2.5185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8363, 'grad_norm': 3.515618085861206, 'learning_rate': 2.4444444444444447e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5163, 'grad_norm': 2.8039231300354004, 'learning_rate': 2.3703703703703707e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5471, 'grad_norm': 3.6498289108276367, 'learning_rate': 2.2962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5162, 'grad_norm': 3.4615838527679443, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7388, 'grad_norm': 3.1166839599609375, 'learning_rate': 2.148148148148148e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8224, 'grad_norm': 2.5952022075653076, 'learning_rate': 2.0740740740740742e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8759, 'grad_norm': 3.30523943901062, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6282, 'grad_norm': 3.076439380645752, 'learning_rate': 1.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3192, 'grad_norm': 2.8034565448760986, 'learning_rate': 1.8518518518518519e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5272, 'grad_norm': 3.195308208465576, 'learning_rate': 1.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3246, 'grad_norm': 2.821117401123047, 'learning_rate': 1.7037037037037038e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5062, 'grad_norm': 3.339019536972046, 'learning_rate': 1.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 5.374, 'grad_norm': 2.916844367980957, 'learning_rate': 1.5555555555555558e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9302, 'grad_norm': 3.1544156074523926, 'learning_rate': 1.4814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9498, 'grad_norm': 2.510974645614624, 'learning_rate': 1.4074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5681, 'grad_norm': 3.5516762733459473, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9176, 'grad_norm': 2.638446807861328, 'learning_rate': 1.2592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 5.738, 'grad_norm': 3.412961483001709, 'learning_rate': 1.1851851851851854e-06, 'epoch': 0.0}\n",
      "{'loss': 4.6651, 'grad_norm': 2.42824387550354, 'learning_rate': 1.111111111111111e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5436, 'grad_norm': 3.3300085067749023, 'learning_rate': 1.0370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3414, 'grad_norm': 2.947559118270874, 'learning_rate': 9.62962962962963e-07, 'epoch': 0.0}\n",
      "{'loss': 5.0102, 'grad_norm': 2.7362563610076904, 'learning_rate': 8.88888888888889e-07, 'epoch': 0.0}\n",
      "{'loss': 5.3019, 'grad_norm': 2.9919047355651855, 'learning_rate': 8.14814814814815e-07, 'epoch': 0.0}\n",
      "{'loss': 5.2664, 'grad_norm': 2.878725290298462, 'learning_rate': 7.407407407407407e-07, 'epoch': 0.0}\n",
      "{'loss': 5.3651, 'grad_norm': 2.5640408992767334, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.0}\n",
      "{'loss': 5.7232, 'grad_norm': 3.2087221145629883, 'learning_rate': 5.925925925925927e-07, 'epoch': 0.0}\n",
      "{'loss': 5.5852, 'grad_norm': 3.0045506954193115, 'learning_rate': 5.185185185185186e-07, 'epoch': 0.0}\n",
      "{'loss': 5.7187, 'grad_norm': 3.440906286239624, 'learning_rate': 4.444444444444445e-07, 'epoch': 0.0}\n",
      "{'loss': 5.2881, 'grad_norm': 3.2752785682678223, 'learning_rate': 3.7037037037037036e-07, 'epoch': 0.0}\n",
      "{'loss': 5.4289, 'grad_norm': 2.7218427658081055, 'learning_rate': 2.9629629629629634e-07, 'epoch': 0.0}\n",
      "{'loss': 5.4034, 'grad_norm': 3.203244686126709, 'learning_rate': 2.2222222222222224e-07, 'epoch': 0.0}\n",
      "{'loss': 5.5927, 'grad_norm': 3.2699527740478516, 'learning_rate': 1.4814814814814817e-07, 'epoch': 0.0}\n",
      "{'train_runtime': 13794.3741, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.011, 'train_loss': 6.720036528905233, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅█▆▄█▃█▆▄▇▇▅▇▆▇▇▅▆▅▄▂▃▄▃▃▃▄▂▂▃▃▃▂▂▃▃▃▁▂▂</td></tr><tr><td>train/learning_rate</td><td>▂▄▅▇███▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▇▄▆▅▆▇▅▆▅▆▅▆▅▅▄▅▄▄▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1131677770655232.0</td></tr><tr><td>train/epoch</td><td>0.00115</td></tr><tr><td>train/global_step</td><td>150</td></tr><tr><td>train/grad_norm</td><td>3.26995</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>5.5927</td></tr><tr><td>train_loss</td><td>6.72004</td></tr><tr><td>train_runtime</td><td>13794.3741</td></tr><tr><td>train_samples_per_second</td><td>0.174</td></tr><tr><td>train_steps_per_second</td><td>0.011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-frog-16</strong> at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento</a><br/> View project at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241129_020131-gp12ento\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-2b-it-abliterated on CookieBaker SFW Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "trainer_sfw.train()\n",
    "trainer_sfw.model.save_pretrained(\"../models/SFW/checkpoint-150\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_sfw = trainer_sfw.model.merge_and_unload()\n",
    "merged_model_sfw.save_pretrained(\"../models/SFW_merged\")\n",
    "tokenizer.save_pretrained(\"../models/SFW_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940ac91e38374cccb881385ef8109edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/899457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "del trainer_sfw\n",
    "\n",
    "trainer_nsfw = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_nsfw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=15,\n",
    "        max_steps=150,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        output_dir=\"../models/NSFW\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f136b7da2dce4dabbe27386c2550767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\CookieBaker-AI\\training\\notebooks\\wandb\\run-20241129_152318-5c5f7111</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/5c5f7111' target=\"_blank\">polar-snow-3</a></strong> to <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/5c5f7111' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/5c5f7111</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55d7f15ce4c473cbb9b54d883390d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0472, 'grad_norm': 5.640249252319336, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.0}\n",
      "{'loss': 9.9103, 'grad_norm': 5.3359575271606445, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.91, 'grad_norm': 5.649725437164307, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 10.6934, 'grad_norm': 6.252277374267578, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 9.1249, 'grad_norm': 4.859716892242432, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 10.6582, 'grad_norm': 6.081592082977295, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 10.8661, 'grad_norm': 6.438713073730469, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 10.2628, 'grad_norm': 5.9638190269470215, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.8225, 'grad_norm': 5.639041900634766, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4904, 'grad_norm': 3.9086406230926514, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 11.0059, 'grad_norm': 6.750884532928467, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 10.2236, 'grad_norm': 6.078712463378906, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 10.4073, 'grad_norm': 6.4061970710754395, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2323, 'grad_norm': 5.147252082824707, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.8267, 'grad_norm': 5.7242584228515625, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 8.9989, 'grad_norm': 5.489227771759033, 'learning_rate': 9.925925925925927e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0606, 'grad_norm': 6.177982330322266, 'learning_rate': 9.851851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0819, 'grad_norm': 6.494633197784424, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0367, 'grad_norm': 6.441848278045654, 'learning_rate': 9.703703703703703e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4789, 'grad_norm': 3.7021360397338867, 'learning_rate': 9.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 10.2468, 'grad_norm': 7.147369861602783, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 8.9964, 'grad_norm': 6.021342754364014, 'learning_rate': 9.481481481481483e-06, 'epoch': 0.0}\n",
      "{'loss': 10.2916, 'grad_norm': 7.185448169708252, 'learning_rate': 9.407407407407408e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2134, 'grad_norm': 6.228518962860107, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5277, 'grad_norm': 5.725605010986328, 'learning_rate': 9.25925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 9.6589, 'grad_norm': 6.788937568664551, 'learning_rate': 9.185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 8.8419, 'grad_norm': 6.104850769042969, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.0}\n",
      "{'loss': 8.7347, 'grad_norm': 5.9596099853515625, 'learning_rate': 9.037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 9.059, 'grad_norm': 6.743227958679199, 'learning_rate': 8.962962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5029, 'grad_norm': 7.28195858001709, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4138, 'grad_norm': 5.00845193862915, 'learning_rate': 8.814814814814817e-06, 'epoch': 0.0}\n",
      "{'loss': 9.1077, 'grad_norm': 6.990591526031494, 'learning_rate': 8.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2882, 'grad_norm': 6.185837268829346, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3422, 'grad_norm': 6.250503063201904, 'learning_rate': 8.592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4368, 'grad_norm': 8.091031074523926, 'learning_rate': 8.518518518518519e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6709, 'grad_norm': 5.61014461517334, 'learning_rate': 8.444444444444446e-06, 'epoch': 0.0}\n",
      "{'loss': 12.5676, 'grad_norm': nan, 'learning_rate': 8.444444444444446e-06, 'epoch': 0.0}\n",
      "{'loss': 8.4588, 'grad_norm': 6.642368793487549, 'learning_rate': 8.37037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6901, 'grad_norm': 6.8221330642700195, 'learning_rate': 8.296296296296297e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9532, 'grad_norm': 5.975425720214844, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5919, 'grad_norm': 7.5021562576293945, 'learning_rate': 8.148148148148148e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6821, 'grad_norm': 5.613699913024902, 'learning_rate': 8.074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5184, 'grad_norm': 6.851784706115723, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5835, 'grad_norm': 6.816142559051514, 'learning_rate': 7.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5867, 'grad_norm': 7.114953517913818, 'learning_rate': 7.851851851851853e-06, 'epoch': 0.0}\n",
      "{'loss': 8.4051, 'grad_norm': 7.23154878616333, 'learning_rate': 7.77777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 7.625, 'grad_norm': 6.267953872680664, 'learning_rate': 7.703703703703704e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9621, 'grad_norm': 6.764136791229248, 'learning_rate': 7.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0419, 'grad_norm': 7.266951560974121, 'learning_rate': 7.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8596, 'grad_norm': 6.200250625610352, 'learning_rate': 7.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2619, 'grad_norm': 7.388625621795654, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5227, 'grad_norm': 6.277222156524658, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7018, 'grad_norm': 6.761936664581299, 'learning_rate': 7.2592592592592605e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6614, 'grad_norm': 7.020855903625488, 'learning_rate': 7.185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3927, 'grad_norm': 6.2798614501953125, 'learning_rate': 7.111111111111112e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5971, 'grad_norm': 6.7986226081848145, 'learning_rate': 7.0370370370370375e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2777, 'grad_norm': 6.312342166900635, 'learning_rate': 6.962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9799, 'grad_norm': 5.679391860961914, 'learning_rate': 6.88888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7762, 'grad_norm': 6.8848981857299805, 'learning_rate': 6.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9183, 'grad_norm': 5.782047748565674, 'learning_rate': 6.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4531, 'grad_norm': 6.658273220062256, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8659, 'grad_norm': 5.420024394989014, 'learning_rate': 6.592592592592592e-06, 'epoch': 0.0}\n",
      "{'loss': 7.024, 'grad_norm': 6.001608848571777, 'learning_rate': 6.51851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1002, 'grad_norm': 5.684037208557129, 'learning_rate': 6.444444444444445e-06, 'epoch': 0.0}\n",
      "{'loss': 7.556, 'grad_norm': 7.070574760437012, 'learning_rate': 6.370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1539, 'grad_norm': 5.825118064880371, 'learning_rate': 6.296296296296297e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1168, 'grad_norm': 5.640423774719238, 'learning_rate': 6.222222222222223e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8565, 'grad_norm': 5.52203369140625, 'learning_rate': 6.148148148148149e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3187, 'grad_norm': 6.662032127380371, 'learning_rate': 6.0740740740740745e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0256, 'grad_norm': 5.759033203125, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 6.93, 'grad_norm': 5.89305305480957, 'learning_rate': 5.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4715, 'grad_norm': 4.813185214996338, 'learning_rate': 5.8518518518518515e-06, 'epoch': 0.0}\n",
      "{'loss': 6.616, 'grad_norm': 4.945833683013916, 'learning_rate': 5.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8983, 'grad_norm': 5.671093463897705, 'learning_rate': 5.7037037037037045e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4838, 'grad_norm': 5.342313289642334, 'learning_rate': 5.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4998, 'grad_norm': 5.441424369812012, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4255, 'grad_norm': 5.718043327331543, 'learning_rate': 5.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0655, 'grad_norm': 6.341522693634033, 'learning_rate': 5.407407407407408e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7047, 'grad_norm': 5.257484436035156, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7353, 'grad_norm': 3.9998867511749268, 'learning_rate': 5.259259259259259e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6304, 'grad_norm': 4.638749122619629, 'learning_rate': 5.185185185185185e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6467, 'grad_norm': 4.8028883934021, 'learning_rate': 5.1111111111111115e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1439, 'grad_norm': 4.700263977050781, 'learning_rate': 5.037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4985, 'grad_norm': 4.846108436584473, 'learning_rate': 4.962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2954, 'grad_norm': 4.647286415100098, 'learning_rate': 4.888888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2902, 'grad_norm': 4.407411098480225, 'learning_rate': 4.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4288, 'grad_norm': 4.418837070465088, 'learning_rate': 4.7407407407407415e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2996, 'grad_norm': 4.373396396636963, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2157, 'grad_norm': 4.256215572357178, 'learning_rate': 4.592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5909, 'grad_norm': 4.704657077789307, 'learning_rate': 4.5185185185185185e-06, 'epoch': 0.0}\n",
      "{'loss': 6.3897, 'grad_norm': 4.423285484313965, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0956, 'grad_norm': 4.194725036621094, 'learning_rate': 4.370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2963, 'grad_norm': 4.5506205558776855, 'learning_rate': 4.296296296296296e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1138, 'grad_norm': 3.7785236835479736, 'learning_rate': 4.222222222222223e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2944, 'grad_norm': 4.52910041809082, 'learning_rate': 4.1481481481481485e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1162, 'grad_norm': 4.410584926605225, 'learning_rate': 4.074074074074074e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1862, 'grad_norm': 4.011416912078857, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0052, 'grad_norm': 4.050644397735596, 'learning_rate': 3.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4206, 'grad_norm': 4.878848552703857, 'learning_rate': 3.851851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0681, 'grad_norm': 4.036311149597168, 'learning_rate': 3.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0232, 'grad_norm': 3.8263726234436035, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1646, 'grad_norm': 4.464507102966309, 'learning_rate': 3.6296296296296302e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1284, 'grad_norm': 3.8286845684051514, 'learning_rate': 3.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1011, 'grad_norm': 3.978696823120117, 'learning_rate': 3.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0629, 'grad_norm': 3.471497058868408, 'learning_rate': 3.4074074074074077e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1376, 'grad_norm': 3.780287742614746, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1086, 'grad_norm': 3.6561033725738525, 'learning_rate': 3.25925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7888, 'grad_norm': 3.619263172149658, 'learning_rate': 3.1851851851851855e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0841, 'grad_norm': 3.8255085945129395, 'learning_rate': 3.1111111111111116e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1441, 'grad_norm': 4.0171732902526855, 'learning_rate': 3.0370370370370372e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0358, 'grad_norm': 3.8204185962677, 'learning_rate': 2.962962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 5.0876, 'grad_norm': 2.985891580581665, 'learning_rate': 2.888888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 6.3169, 'grad_norm': 4.050252437591553, 'learning_rate': 2.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0414, 'grad_norm': 3.5812759399414062, 'learning_rate': 2.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 5.717, 'grad_norm': 3.775737762451172, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7405, 'grad_norm': 3.6585705280303955, 'learning_rate': 2.5925925925925925e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8802, 'grad_norm': 3.8140106201171875, 'learning_rate': 2.5185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7119, 'grad_norm': 3.5611367225646973, 'learning_rate': 2.4444444444444447e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7489, 'grad_norm': 3.3441193103790283, 'learning_rate': 2.3703703703703707e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7368, 'grad_norm': 3.6451096534729004, 'learning_rate': 2.2962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9407, 'grad_norm': 3.5076589584350586, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8608, 'grad_norm': 3.538865804672241, 'learning_rate': 2.148148148148148e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0291, 'grad_norm': 3.9734456539154053, 'learning_rate': 2.0740740740740742e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0368, 'grad_norm': 4.064731597900391, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9829, 'grad_norm': 4.28141975402832, 'learning_rate': 1.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9572, 'grad_norm': 3.544639825820923, 'learning_rate': 1.8518518518518519e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0588, 'grad_norm': 3.8584654331207275, 'learning_rate': 1.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 5.947, 'grad_norm': 4.332820892333984, 'learning_rate': 1.7037037037037038e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8705, 'grad_norm': 3.6252541542053223, 'learning_rate': 1.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5231, 'grad_norm': 2.829103469848633, 'learning_rate': 1.5555555555555558e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8803, 'grad_norm': 3.8421010971069336, 'learning_rate': 1.4814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5639, 'grad_norm': 3.905120372772217, 'learning_rate': 1.4074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4512, 'grad_norm': 2.924635648727417, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6486, 'grad_norm': 4.225833892822266, 'learning_rate': 1.2592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6028, 'grad_norm': 3.1741812229156494, 'learning_rate': 1.1851851851851854e-06, 'epoch': 0.0}\n",
      "{'loss': 5.348, 'grad_norm': 2.706515312194824, 'learning_rate': 1.111111111111111e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9326, 'grad_norm': 3.2014267444610596, 'learning_rate': 1.0370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6877, 'grad_norm': 3.8718209266662598, 'learning_rate': 9.62962962962963e-07, 'epoch': 0.0}\n",
      "{'loss': 5.4477, 'grad_norm': 3.7178237438201904, 'learning_rate': 8.88888888888889e-07, 'epoch': 0.0}\n",
      "{'loss': 5.7587, 'grad_norm': 3.86826229095459, 'learning_rate': 8.14814814814815e-07, 'epoch': 0.0}\n",
      "{'loss': 5.9845, 'grad_norm': 3.3556106090545654, 'learning_rate': 7.407407407407407e-07, 'epoch': 0.0}\n",
      "{'loss': 5.593, 'grad_norm': 3.1089587211608887, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.0}\n",
      "{'loss': 5.9652, 'grad_norm': 3.935248613357544, 'learning_rate': 5.925925925925927e-07, 'epoch': 0.0}\n",
      "{'loss': 5.6866, 'grad_norm': 2.707876205444336, 'learning_rate': 5.185185185185186e-07, 'epoch': 0.0}\n",
      "{'loss': 6.0409, 'grad_norm': 3.879394292831421, 'learning_rate': 4.444444444444445e-07, 'epoch': 0.0}\n",
      "{'loss': 5.8986, 'grad_norm': 4.214433670043945, 'learning_rate': 3.7037037037037036e-07, 'epoch': 0.0}\n",
      "{'loss': 5.8893, 'grad_norm': 3.366825819015503, 'learning_rate': 2.9629629629629634e-07, 'epoch': 0.0}\n",
      "{'loss': 5.6779, 'grad_norm': 3.4596517086029053, 'learning_rate': 2.2222222222222224e-07, 'epoch': 0.0}\n",
      "{'loss': 5.7266, 'grad_norm': 3.619464159011841, 'learning_rate': 1.4814814814814817e-07, 'epoch': 0.0}\n",
      "{'loss': 5.8745, 'grad_norm': 3.2892532348632812, 'learning_rate': 7.407407407407409e-08, 'epoch': 0.0}\n",
      "{'train_runtime': 8123.9842, 'train_samples_per_second': 0.295, 'train_steps_per_second': 0.018, 'train_loss': 7.260268882115682, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▆▅▆▅▄█▅▅▇▇▆▆▆▅▅▅▅▆▅▄▃▄▃▄▃▃▂▃▂▂▂▂▂▃▁▁▁▂▂▂</td></tr><tr><td>train/learning_rate</td><td>▁▃▅███▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇▇█▄▆▇▆▆▆▅▅▅▄▃▃▃▃▃▃▃▂▂▂▁▃▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>944263365714432.0</td></tr><tr><td>train/epoch</td><td>0.00267</td></tr><tr><td>train/global_step</td><td>150</td></tr><tr><td>train/grad_norm</td><td>3.28925</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>5.8745</td></tr><tr><td>train_loss</td><td>7.26027</td></tr><tr><td>train_runtime</td><td>8123.9842</td></tr><tr><td>train_samples_per_second</td><td>0.295</td></tr><tr><td>train_steps_per_second</td><td>0.018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-snow-3</strong> at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/5c5f7111' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/5c5f7111</a><br/> View project at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241129_152318-5c5f7111\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-2b-it-abliterated on CookieBaker NSFW Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "trainer_nsfw.train()\n",
    "trainer_nsfw.model.save_pretrained(\"../models/NSFW/checkpoint-150\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\peft\\tuners\\lora\\bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/NSFW_merged\\\\tokenizer_config.json',\n",
       " '../models/NSFW_merged\\\\special_tokens_map.json',\n",
       " '../models/NSFW_merged\\\\tokenizer.model',\n",
       " '../models/NSFW_merged\\\\added_tokens.json',\n",
       " '../models/NSFW_merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model_nsfw = trainer_nsfw.model.merge_and_unload()\n",
    "merged_model_nsfw.save_pretrained(\"../models/NSFW_merged\")\n",
    "tokenizer.save_pretrained(\"../models/NSFW_merged\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
