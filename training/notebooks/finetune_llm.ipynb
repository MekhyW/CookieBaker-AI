{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmekhyw\u001b[0m (\u001b[33mmekhyw-insper\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\felip\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6885a58c9034587a21ff7d88d143ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = \"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_name = \"IlyaGusev/gemma-2-2b-it-abliterated\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_sfw = datasets.load_dataset(\"parquet\", data_files=\"../data/SFW_qa.parquet\")\n",
    "dataset_nsfw = datasets.load_dataset(\"parquet\", data_files=\"../data/NSFW_qa.parquet\")\n",
    "dataset_sfw = dataset_sfw.shuffle(seed=42)\n",
    "dataset_nsfw = dataset_nsfw.shuffle(seed=42)\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    def format_chat(example):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example['query']},\n",
    "            {\"role\": \"assistant\", \"content\": example['response']}\n",
    "        ]\n",
    "        formatted_chat = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        return {\"text\": formatted_chat}\n",
    "    formatted_dataset = dataset.map(format_chat)\n",
    "    formatted_dataset = formatted_dataset['train'].remove_columns(\n",
    "        [col for col in formatted_dataset['train'].column_names if col != \"text\"]\n",
    "    )\n",
    "    return formatted_dataset\n",
    "\n",
    "dataset_sfw = prepare_dataset(dataset_sfw)\n",
    "dataset_nsfw = prepare_dataset(dataset_nsfw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2090473\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 899457\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nsfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer_sfw = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_sfw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=5,\n",
    "        max_steps=100,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        output_dir=\"../models/SFW\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\CookieBaker-AI\\training\\notebooks\\wandb\\run-20241127_230606-qurneru8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/qurneru8' target=\"_blank\">dainty-forest-12</a></strong> to <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/qurneru8' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/qurneru8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534ecad324a485a92b7a96d5434f7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.303, 'grad_norm': 5.055233478546143, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2915, 'grad_norm': 4.2177886962890625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5407, 'grad_norm': 4.7495198249816895, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 8.342, 'grad_norm': 4.732071399688721, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.411, 'grad_norm': nan, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2133, 'grad_norm': 5.214311122894287, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 8.3173, 'grad_norm': 4.42290735244751, 'learning_rate': 9.894736842105264e-06, 'epoch': 0.0}\n",
      "{'loss': 10.3201, 'grad_norm': 6.658257484436035, 'learning_rate': 9.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 8.7428, 'grad_norm': 5.252752780914307, 'learning_rate': 9.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4293, 'grad_norm': 5.69586706161499, 'learning_rate': 9.578947368421054e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2592, 'grad_norm': 5.565417766571045, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.0}\n",
      "{'loss': 9.0882, 'grad_norm': 5.478926658630371, 'learning_rate': 9.36842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 9.1981, 'grad_norm': 5.838810443878174, 'learning_rate': 9.263157894736842e-06, 'epoch': 0.0}\n",
      "{'loss': 9.8557, 'grad_norm': 6.73486328125, 'learning_rate': 9.157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5539, 'grad_norm': 6.073154449462891, 'learning_rate': 9.05263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 8.4214, 'grad_norm': 5.735259532928467, 'learning_rate': 8.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5836, 'grad_norm': 3.3340885639190674, 'learning_rate': 8.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 8.475, 'grad_norm': 5.548854351043701, 'learning_rate': 8.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0636, 'grad_norm': 4.757812976837158, 'learning_rate': 8.631578947368422e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2072, 'grad_norm': 4.399514198303223, 'learning_rate': 8.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9704, 'grad_norm': 5.158522605895996, 'learning_rate': 8.421052631578948e-06, 'epoch': 0.0}\n",
      "{'loss': 9.1238, 'grad_norm': 6.7515645027160645, 'learning_rate': 8.315789473684212e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7554, 'grad_norm': 4.841698169708252, 'learning_rate': 8.210526315789475e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4513, 'grad_norm': 3.833142042160034, 'learning_rate': 8.105263157894736e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3028, 'grad_norm': 6.785885334014893, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0192, 'grad_norm': 5.293527603149414, 'learning_rate': 7.894736842105265e-06, 'epoch': 0.0}\n",
      "{'loss': 9.044, 'grad_norm': 6.94694185256958, 'learning_rate': 7.789473684210526e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4325, 'grad_norm': 5.266158580780029, 'learning_rate': 7.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7283, 'grad_norm': 5.405332565307617, 'learning_rate': 7.578947368421054e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5541, 'grad_norm': 5.888077259063721, 'learning_rate': 7.473684210526316e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8034, 'grad_norm': 5.320357322692871, 'learning_rate': 7.368421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0444, 'grad_norm': 4.718094825744629, 'learning_rate': 7.263157894736843e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2842, 'grad_norm': 4.410869121551514, 'learning_rate': 7.157894736842106e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6233, 'grad_norm': 5.286590576171875, 'learning_rate': 7.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1956, 'grad_norm': 6.1251139640808105, 'learning_rate': 6.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0323, 'grad_norm': 6.085124969482422, 'learning_rate': 6.842105263157896e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0321, 'grad_norm': 5.802402496337891, 'learning_rate': 6.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 8.636, 'grad_norm': 6.876945972442627, 'learning_rate': 6.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7, 'grad_norm': 5.892759799957275, 'learning_rate': 6.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4925, 'grad_norm': 5.581000328063965, 'learning_rate': 6.421052631578948e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9121, 'grad_norm': 6.049129009246826, 'learning_rate': 6.31578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 8.109, 'grad_norm': 6.650177955627441, 'learning_rate': 6.2105263157894745e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1006, 'grad_norm': 5.1621270179748535, 'learning_rate': 6.105263157894738e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3031, 'grad_norm': 5.522348403930664, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8467, 'grad_norm': 6.008897304534912, 'learning_rate': 5.8947368421052634e-06, 'epoch': 0.0}\n",
      "{'loss': 7.378, 'grad_norm': 6.129851818084717, 'learning_rate': 5.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0651, 'grad_norm': 5.204965114593506, 'learning_rate': 5.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4117, 'grad_norm': 5.873230457305908, 'learning_rate': 5.578947368421052e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9799, 'grad_norm': 6.7435712814331055, 'learning_rate': 5.4736842105263165e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1706, 'grad_norm': 5.534829139709473, 'learning_rate': 5.36842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2383, 'grad_norm': 5.784252643585205, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3285, 'grad_norm': 6.506998538970947, 'learning_rate': 5.157894736842106e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4998, 'grad_norm': 5.981656074523926, 'learning_rate': 5.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5363, 'grad_norm': nan, 'learning_rate': 5.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2232, 'grad_norm': 5.821463108062744, 'learning_rate': 4.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2126, 'grad_norm': 6.124855041503906, 'learning_rate': 4.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4917, 'grad_norm': 6.408603668212891, 'learning_rate': 4.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6221, 'grad_norm': 6.3508195877075195, 'learning_rate': 4.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9212, 'grad_norm': 4.868586540222168, 'learning_rate': 4.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7319, 'grad_norm': 4.98696756362915, 'learning_rate': 4.4210526315789476e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8074, 'grad_norm': 5.348174095153809, 'learning_rate': 4.315789473684211e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8548, 'grad_norm': 5.5963239669799805, 'learning_rate': 4.210526315789474e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5045, 'grad_norm': 5.042436599731445, 'learning_rate': 4.105263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3873, 'grad_norm': 6.2903900146484375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1183, 'grad_norm': 5.934410095214844, 'learning_rate': 3.894736842105263e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9642, 'grad_norm': 5.436816692352295, 'learning_rate': 3.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1527, 'grad_norm': 6.165295124053955, 'learning_rate': 3.6842105263157896e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7868, 'grad_norm': 5.5192766189575195, 'learning_rate': 3.578947368421053e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2995, 'grad_norm': 6.611073970794678, 'learning_rate': 3.473684210526316e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2695, 'grad_norm': 4.842761039733887, 'learning_rate': 3.368421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9313, 'grad_norm': 6.116581439971924, 'learning_rate': 3.2631578947368423e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4199, 'grad_norm': 4.875024318695068, 'learning_rate': 3.157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9286, 'grad_norm': 5.731195449829102, 'learning_rate': 3.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8302, 'grad_norm': 5.896537780761719, 'learning_rate': 2.9473684210526317e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6685, 'grad_norm': 5.21926736831665, 'learning_rate': 2.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9847, 'grad_norm': 5.969410419464111, 'learning_rate': 2.7368421052631583e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4351, 'grad_norm': 4.825680255889893, 'learning_rate': 2.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1392, 'grad_norm': 6.216092586517334, 'learning_rate': 2.5263157894736844e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0475, 'grad_norm': 5.5854949951171875, 'learning_rate': 2.4210526315789477e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0272, 'grad_norm': 6.3486809730529785, 'learning_rate': 2.3157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6095, 'grad_norm': 4.5949907302856445, 'learning_rate': 2.2105263157894738e-06, 'epoch': 0.0}\n",
      "{'loss': 6.215, 'grad_norm': 4.425053596496582, 'learning_rate': 2.105263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8899, 'grad_norm': 5.238925933837891, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 6.3965, 'grad_norm': 5.014461517333984, 'learning_rate': 1.8947368421052634e-06, 'epoch': 0.0}\n",
      "{'loss': 5.0833, 'grad_norm': 3.3814477920532227, 'learning_rate': 1.7894736842105265e-06, 'epoch': 0.0}\n",
      "{'loss': 6.3131, 'grad_norm': 4.633682727813721, 'learning_rate': 1.6842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5485, 'grad_norm': 5.365000247955322, 'learning_rate': 1.5789473684210526e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9397, 'grad_norm': 4.211307525634766, 'learning_rate': 1.4736842105263159e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4911, 'grad_norm': 5.271914005279541, 'learning_rate': 1.3684210526315791e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8426, 'grad_norm': 4.301763534545898, 'learning_rate': 1.2631578947368422e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1382, 'grad_norm': 4.9831223487854, 'learning_rate': 1.1578947368421053e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4363, 'grad_norm': 4.7019453048706055, 'learning_rate': 1.0526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5492, 'grad_norm': 5.328712463378906, 'learning_rate': 9.473684210526317e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7004, 'grad_norm': 5.583320140838623, 'learning_rate': 8.421052631578948e-07, 'epoch': 0.0}\n",
      "{'loss': 6.5739, 'grad_norm': 5.0626373291015625, 'learning_rate': 7.368421052631579e-07, 'epoch': 0.0}\n",
      "{'loss': 6.5407, 'grad_norm': 4.52918004989624, 'learning_rate': 6.315789473684211e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8999, 'grad_norm': 5.5578293800354, 'learning_rate': 5.263157894736843e-07, 'epoch': 0.0}\n",
      "{'loss': 5.9772, 'grad_norm': 4.365411281585693, 'learning_rate': 4.210526315789474e-07, 'epoch': 0.0}\n",
      "{'loss': 6.3982, 'grad_norm': 4.845185279846191, 'learning_rate': 3.1578947368421055e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8065, 'grad_norm': 5.385493755340576, 'learning_rate': 2.105263157894737e-07, 'epoch': 0.0}\n",
      "{'train_runtime': 11842.6837, 'train_samples_per_second': 0.135, 'train_steps_per_second': 0.008, 'train_loss': 7.4583201551437375, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▁▂ █▄▆▅▃▂█▄▄▄▆▆▅▆▆▅▆▇▆▅▆▇▄▃▆█▃▆▇▅▇▂▁▁▂▅▄</td></tr><tr><td>train/learning_rate</td><td>▅▇████▇▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆▆▆██▆▆▅▅▂█▄▅▆▅▅▄▆▃▄▄▅▄▄▄▄▃▃▂▃▃▃▃▂▃▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>764821703863296.0</td></tr><tr><td>train/epoch</td><td>0.00077</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>5.38549</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>6.8065</td></tr><tr><td>train_loss</td><td>7.45832</td></tr><tr><td>train_runtime</td><td>11842.6837</td></tr><tr><td>train_samples_per_second</td><td>0.135</td></tr><tr><td>train_steps_per_second</td><td>0.008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-forest-12</strong> at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/qurneru8' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/qurneru8</a><br/> View project at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241127_230606-qurneru8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-2b-it-abliterated on CookieBaker SFW Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "trainer_sfw.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4511bfe5c5234ddf969b31741112b830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/899457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "del trainer_sfw\n",
    "\n",
    "trainer_nsfw = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_nsfw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=5,\n",
    "        max_steps=100,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        output_dir=\"../models/NSFW\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe5013f3f1b40dd9b7f81540772bcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\CookieBaker-AI\\training\\notebooks\\wandb\\run-20241128_034550-gkoeoj5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e' target=\"_blank\">dutiful-snowflake-2</a></strong> to <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07af52739017405288aaa0840e384ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0472, 'grad_norm': 5.640249252319336, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9103, 'grad_norm': 5.3359575271606445, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9043, 'grad_norm': 5.645761966705322, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 10.6738, 'grad_norm': 6.25145149230957, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.0942, 'grad_norm': 4.865102767944336, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 10.5909, 'grad_norm': 6.099635601043701, 'learning_rate': 9.894736842105264e-06, 'epoch': 0.0}\n",
      "{'loss': 10.7541, 'grad_norm': 6.482822418212891, 'learning_rate': 9.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 10.1237, 'grad_norm': 6.0161590576171875, 'learning_rate': 9.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 9.6602, 'grad_norm': 5.7317585945129395, 'learning_rate': 9.578947368421054e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3626, 'grad_norm': 3.9530227184295654, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.0}\n",
      "{'loss': 10.7523, 'grad_norm': 6.8787522315979, 'learning_rate': 9.36842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9793, 'grad_norm': 6.155326843261719, 'learning_rate': 9.263157894736842e-06, 'epoch': 0.0}\n",
      "{'loss': 10.1341, 'grad_norm': 6.502983093261719, 'learning_rate': 9.157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 9.0086, 'grad_norm': 5.242326259613037, 'learning_rate': 9.05263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5701, 'grad_norm': 5.857893943786621, 'learning_rate': 8.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 8.7513, 'grad_norm': 5.6017560958862305, 'learning_rate': 8.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 9.7836, 'grad_norm': 6.31508731842041, 'learning_rate': 8.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 9.8008, 'grad_norm': 6.612189292907715, 'learning_rate': 8.631578947368422e-06, 'epoch': 0.0}\n",
      "{'loss': 9.7586, 'grad_norm': 6.52145528793335, 'learning_rate': 8.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 6.328, 'grad_norm': 3.751744508743286, 'learning_rate': 8.421052631578948e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9537, 'grad_norm': 7.247509479522705, 'learning_rate': 8.315789473684212e-06, 'epoch': 0.0}\n",
      "{'loss': 8.7607, 'grad_norm': 6.077712535858154, 'learning_rate': 8.210526315789475e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0158, 'grad_norm': 7.250381946563721, 'learning_rate': 8.105263157894736e-06, 'epoch': 0.0}\n",
      "{'loss': 8.9885, 'grad_norm': 6.287909507751465, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3258, 'grad_norm': 5.741156101226807, 'learning_rate': 7.894736842105265e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4265, 'grad_norm': 6.8032002449035645, 'learning_rate': 7.789473684210526e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6462, 'grad_norm': 6.104853630065918, 'learning_rate': 7.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5549, 'grad_norm': 5.974133014678955, 'learning_rate': 7.578947368421054e-06, 'epoch': 0.0}\n",
      "{'loss': 8.8603, 'grad_norm': 6.735897064208984, 'learning_rate': 7.473684210526316e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3028, 'grad_norm': 7.271637439727783, 'learning_rate': 7.368421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2866, 'grad_norm': 4.994191646575928, 'learning_rate': 7.263157894736843e-06, 'epoch': 0.0}\n",
      "{'loss': 8.9425, 'grad_norm': 6.951260089874268, 'learning_rate': 7.157894736842106e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1539, 'grad_norm': 6.145110607147217, 'learning_rate': 7.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2207, 'grad_norm': 6.2005934715271, 'learning_rate': 6.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2947, 'grad_norm': 7.999884128570557, 'learning_rate': 6.842105263157896e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5838, 'grad_norm': 5.5475993156433105, 'learning_rate': 6.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0581, 'grad_norm': 6.17701530456543, 'learning_rate': 6.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3187, 'grad_norm': 6.566706657409668, 'learning_rate': 6.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5625, 'grad_norm': 6.761594295501709, 'learning_rate': 6.421052631578948e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8564, 'grad_norm': 5.905500888824463, 'learning_rate': 6.31578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 8.484, 'grad_norm': 7.402847766876221, 'learning_rate': 6.2105263157894745e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6174, 'grad_norm': 5.558272361755371, 'learning_rate': 6.105263157894738e-06, 'epoch': 0.0}\n",
      "{'loss': 8.4544, 'grad_norm': 6.783658981323242, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5365, 'grad_norm': 6.750698089599609, 'learning_rate': 5.8947368421052634e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5562, 'grad_norm': 7.054347038269043, 'learning_rate': 5.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3933, 'grad_norm': 7.174205303192139, 'learning_rate': 5.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6301, 'grad_norm': 6.223912715911865, 'learning_rate': 5.578947368421052e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9856, 'grad_norm': 6.72188663482666, 'learning_rate': 5.4736842105263165e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0845, 'grad_norm': 7.234642505645752, 'learning_rate': 5.36842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9109, 'grad_norm': 6.163455963134766, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3442, 'grad_norm': 7.36458158493042, 'learning_rate': 5.157894736842106e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6125, 'grad_norm': 6.300682067871094, 'learning_rate': 5.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8155, 'grad_norm': 6.794142246246338, 'learning_rate': 4.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7995, 'grad_norm': 7.065849781036377, 'learning_rate': 4.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5334, 'grad_norm': 6.30277156829834, 'learning_rate': 4.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7645, 'grad_norm': 6.848322868347168, 'learning_rate': 4.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4624, 'grad_norm': 6.424248218536377, 'learning_rate': 4.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1586, 'grad_norm': 5.770240783691406, 'learning_rate': 4.4210526315789476e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0039, 'grad_norm': 6.985846042633057, 'learning_rate': 4.315789473684211e-06, 'epoch': 0.0}\n",
      "{'loss': 7.13, 'grad_norm': 5.9023051261901855, 'learning_rate': 4.210526315789474e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7187, 'grad_norm': 6.839781284332275, 'learning_rate': 4.105263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0968, 'grad_norm': 5.6195292472839355, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2991, 'grad_norm': 6.225754261016846, 'learning_rate': 3.894736842105263e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3844, 'grad_norm': 5.980795383453369, 'learning_rate': 3.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9404, 'grad_norm': 7.404062747955322, 'learning_rate': 3.6842105263157896e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4668, 'grad_norm': 6.115976333618164, 'learning_rate': 3.578947368421053e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4507, 'grad_norm': 5.997700214385986, 'learning_rate': 3.473684210526316e-06, 'epoch': 0.0}\n",
      "{'loss': 7.207, 'grad_norm': 5.956457614898682, 'learning_rate': 3.368421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7513, 'grad_norm': 7.1224517822265625, 'learning_rate': 3.2631578947368423e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4178, 'grad_norm': 6.216053009033203, 'learning_rate': 3.157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3703, 'grad_norm': 6.576999187469482, 'learning_rate': 3.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8553, 'grad_norm': 5.528324127197266, 'learning_rate': 2.9473684210526317e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0177, 'grad_norm': 5.759592533111572, 'learning_rate': 2.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3719, 'grad_norm': 6.481958866119385, 'learning_rate': 2.7368421052631583e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9652, 'grad_norm': 6.437648773193359, 'learning_rate': 2.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0061, 'grad_norm': 6.282752990722656, 'learning_rate': 2.5263157894736844e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9764, 'grad_norm': 6.8480095863342285, 'learning_rate': 2.4210526315789477e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6782, 'grad_norm': 7.648647785186768, 'learning_rate': 2.3157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2407, 'grad_norm': 6.305149078369141, 'learning_rate': 2.2105263157894738e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1558, 'grad_norm': 4.783631801605225, 'learning_rate': 2.105263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1595, 'grad_norm': 5.74940824508667, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2115, 'grad_norm': 5.990188121795654, 'learning_rate': 1.8947368421052634e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6953, 'grad_norm': 5.920891284942627, 'learning_rate': 1.7894736842105265e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0951, 'grad_norm': 6.066863536834717, 'learning_rate': 1.6842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8988, 'grad_norm': 5.827770709991455, 'learning_rate': 1.5789473684210526e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8437, 'grad_norm': 5.659218788146973, 'learning_rate': 1.4736842105263159e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0453, 'grad_norm': 5.702203750610352, 'learning_rate': 1.3684210526315791e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8899, 'grad_norm': 5.572003364562988, 'learning_rate': 1.2631578947368422e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8307, 'grad_norm': 5.5450544357299805, 'learning_rate': 1.1578947368421053e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2495, 'grad_norm': 5.988315105438232, 'learning_rate': 1.0526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0681, 'grad_norm': 6.012745380401611, 'learning_rate': 9.473684210526317e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7353, 'grad_norm': 5.570369720458984, 'learning_rate': 8.421052631578948e-07, 'epoch': 0.0}\n",
      "{'loss': 7.0446, 'grad_norm': 6.087926864624023, 'learning_rate': 7.368421052631579e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7244, 'grad_norm': 5.016287326812744, 'learning_rate': 6.315789473684211e-07, 'epoch': 0.0}\n",
      "{'loss': 7.0284, 'grad_norm': 5.956068515777588, 'learning_rate': 5.263157894736843e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8571, 'grad_norm': 5.817139625549316, 'learning_rate': 4.210526315789474e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8843, 'grad_norm': 5.456579208374023, 'learning_rate': 3.1578947368421055e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7411, 'grad_norm': 5.519063949584961, 'learning_rate': 2.105263157894737e-07, 'epoch': 0.0}\n",
      "{'loss': 7.2883, 'grad_norm': 6.552621841430664, 'learning_rate': 1.0526315789473685e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8105, 'grad_norm': 5.3651347160339355, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 12707.3864, 'train_samples_per_second': 0.126, 'train_steps_per_second': 0.008, 'train_loss': 8.078502898216248, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▃▅▅▁▅▅▃▄▄▇▅▅▅▄▅█▆▆▄▆▆▇▆▆▅▄▄▅▅▇▅▂▄▄▄▅▅▅▃</td></tr><tr><td>train/learning_rate</td><td>▄▅████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇▇█▃▇▇▅▇▆▅▆▃▄▄▅▅▅▅▄▃▃▄▃▃▃▃▃▃▂▂▂▁▃▂▂▃▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>630481593272832.0</td></tr><tr><td>train/epoch</td><td>0.00178</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>5.36513</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>6.8105</td></tr><tr><td>train_loss</td><td>8.0785</td></tr><tr><td>train_runtime</td><td>12707.3864</td></tr><tr><td>train_samples_per_second</td><td>0.126</td></tr><tr><td>train_steps_per_second</td><td>0.008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-snowflake-2</strong> at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e</a><br/> View project at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_034550-gkoeoj5e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-2b-it-abliterated on CookieBaker NSFW Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "trainer_nsfw.train()\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
