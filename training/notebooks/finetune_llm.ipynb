{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmekhyw\u001b[0m (\u001b[33mmekhyw-insper\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\felip\\_netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../../.env\")\n",
    "\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\scipy\\__init__.py:169: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1251fed74b74e1a83919d36b0366518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_compute_dtype = \"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_name = \"IlyaGusev/gemma-2-2b-it-abliterated\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type = \"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    def format_chat(example):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example['query']},\n",
    "            {\"role\": \"assistant\", \"content\": example['response']}\n",
    "        ]\n",
    "        formatted_chat = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        return {\"text\": formatted_chat}\n",
    "    formatted_dataset = dataset.map(format_chat)\n",
    "    formatted_dataset = formatted_dataset['train'].remove_columns(\n",
    "        [col for col in formatted_dataset['train'].column_names if col != \"text\"]\n",
    "    )\n",
    "    return formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 2090473\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sfw = datasets.load_dataset(\"parquet\", data_files=\"../data/SFW_qa.parquet\")\n",
    "dataset_sfw = dataset_sfw.shuffle(seed=42)\n",
    "dataset_sfw = prepare_dataset(dataset_sfw)\n",
    "dataset_sfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 899457\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_nsfw = datasets.load_dataset(\"parquet\", data_files=\"../data/NSFW_qa.parquet\")\n",
    "dataset_nsfw = dataset_nsfw.shuffle(seed=42)\n",
    "dataset_nsfw = prepare_dataset(dataset_nsfw)\n",
    "dataset_nsfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\trl\\trainer\\sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer_sfw = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_sfw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=15,\n",
    "        max_steps=150,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        output_dir=\"../models/SFW\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.cuda.init()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\CookieBaker-AI\\training\\notebooks\\wandb\\run-20241129_020131-gp12ento</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento' target=\"_blank\">dutiful-frog-16</a></strong> to <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17091d698b874261bb58e0557f436fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.303, 'grad_norm': 4.914122581481934, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.0}\n",
      "{'loss': 8.2915, 'grad_norm': 4.106173515319824, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5451, 'grad_norm': 4.586526393890381, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3551, 'grad_norm': 4.605806827545166, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 8.44, 'grad_norm': 4.779813766479492, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 9.235, 'grad_norm': 5.068270683288574, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3515, 'grad_norm': 4.300962924957275, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 10.4071, 'grad_norm': 6.471405982971191, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 8.834, 'grad_norm': 4.997831344604492, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5594, 'grad_norm': 5.541932582855225, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4003, 'grad_norm': 5.3805251121521, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2469, 'grad_norm': 5.334012508392334, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3799, 'grad_norm': 5.680000305175781, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0755, 'grad_norm': 6.499693870544434, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.7534, 'grad_norm': 5.807558059692383, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 8.6168, 'grad_norm': 5.530555725097656, 'learning_rate': 9.925925925925927e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6854, 'grad_norm': 3.17997145652771, 'learning_rate': 9.851851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6531, 'grad_norm': 5.3292975425720215, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2118, 'grad_norm': 4.616573810577393, 'learning_rate': 9.703703703703703e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3419, 'grad_norm': 4.261197090148926, 'learning_rate': 9.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1244, 'grad_norm': 5.016082286834717, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3252, 'grad_norm': 6.5429368019104, 'learning_rate': 9.481481481481483e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8916, 'grad_norm': 4.747773170471191, 'learning_rate': 9.407407407407408e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5566, 'grad_norm': 3.7492105960845947, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4832, 'grad_norm': 6.656974792480469, 'learning_rate': 9.25925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1505, 'grad_norm': 5.176398277282715, 'learning_rate': 9.185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2091, 'grad_norm': 6.8035078048706055, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5524, 'grad_norm': 5.163696765899658, 'learning_rate': 9.037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 7.842, 'grad_norm': 5.270222187042236, 'learning_rate': 8.962962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6657, 'grad_norm': 5.733499050140381, 'learning_rate': 8.888888888888888e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8987, 'grad_norm': 5.23502779006958, 'learning_rate': 8.814814814814817e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1228, 'grad_norm': 4.630730628967285, 'learning_rate': 8.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3445, 'grad_norm': 4.345437526702881, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.0}\n",
      "{'loss': 7.692, 'grad_norm': 5.195389270782471, 'learning_rate': 8.592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2675, 'grad_norm': 6.0369648933410645, 'learning_rate': 8.518518518518519e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0948, 'grad_norm': 5.991506576538086, 'learning_rate': 8.444444444444446e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0787, 'grad_norm': 5.727706432342529, 'learning_rate': 8.37037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6768, 'grad_norm': 6.803436279296875, 'learning_rate': 8.296296296296297e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7225, 'grad_norm': 5.808382987976074, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5067, 'grad_norm': 5.463085651397705, 'learning_rate': 8.148148148148148e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9161, 'grad_norm': 5.968760967254639, 'learning_rate': 8.074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1011, 'grad_norm': nan, 'learning_rate': 8.074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1256, 'grad_norm': 5.110297203063965, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3246, 'grad_norm': 5.460613250732422, 'learning_rate': 7.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8548, 'grad_norm': 5.898072242736816, 'learning_rate': 7.851851851851853e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3731, 'grad_norm': 5.975387096405029, 'learning_rate': 7.77777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0464, 'grad_norm': 5.117428302764893, 'learning_rate': 7.703703703703704e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3814, 'grad_norm': 5.755771636962891, 'learning_rate': 7.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9231, 'grad_norm': 6.621214389801025, 'learning_rate': 7.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1117, 'grad_norm': 5.408539772033691, 'learning_rate': 7.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1625, 'grad_norm': 5.642579555511475, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2217, 'grad_norm': 6.34633207321167, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 7.394, 'grad_norm': 5.812069892883301, 'learning_rate': 7.2592592592592605e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4181, 'grad_norm': nan, 'learning_rate': 7.2592592592592605e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0971, 'grad_norm': 5.618387222290039, 'learning_rate': 7.185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 7.068, 'grad_norm': 5.955078601837158, 'learning_rate': 7.111111111111112e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3285, 'grad_norm': 6.2190752029418945, 'learning_rate': 7.0370370370370375e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4479, 'grad_norm': 6.063862323760986, 'learning_rate': 6.962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7654, 'grad_norm': 4.723182678222656, 'learning_rate': 6.88888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5648, 'grad_norm': 4.796474456787109, 'learning_rate': 6.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6181, 'grad_norm': 5.125892162322998, 'learning_rate': 6.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6356, 'grad_norm': 5.393930912017822, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2953, 'grad_norm': 4.747994422912598, 'learning_rate': 6.592592592592592e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1058, 'grad_norm': 5.898232936859131, 'learning_rate': 6.51851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8395, 'grad_norm': 5.573493003845215, 'learning_rate': 6.444444444444445e-06, 'epoch': 0.0}\n",
      "{'loss': 6.7017, 'grad_norm': 5.055373668670654, 'learning_rate': 6.370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8297, 'grad_norm': 5.677459716796875, 'learning_rate': 6.296296296296297e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4863, 'grad_norm': 5.001908302307129, 'learning_rate': 6.222222222222223e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9306, 'grad_norm': 5.986841201782227, 'learning_rate': 6.148148148148149e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9721, 'grad_norm': 4.351851940155029, 'learning_rate': 6.0740740740740745e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5415, 'grad_norm': 5.399590492248535, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0908, 'grad_norm': 4.37050199508667, 'learning_rate': 5.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5465, 'grad_norm': 5.043421745300293, 'learning_rate': 5.8518518518518515e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4116, 'grad_norm': 5.020814895629883, 'learning_rate': 5.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 6.29, 'grad_norm': 4.505062103271484, 'learning_rate': 5.7037037037037045e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5421, 'grad_norm': 5.0331292152404785, 'learning_rate': 5.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0529, 'grad_norm': 4.157555103302002, 'learning_rate': 5.555555555555557e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6469, 'grad_norm': 5.222238540649414, 'learning_rate': 5.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5831, 'grad_norm': 4.665149688720703, 'learning_rate': 5.407407407407408e-06, 'epoch': 0.0}\n",
      "{'loss': 6.5081, 'grad_norm': 5.100517272949219, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2121, 'grad_norm': 3.765774726867676, 'learning_rate': 5.259259259259259e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8025, 'grad_norm': 3.6263539791107178, 'learning_rate': 5.185185185185185e-06, 'epoch': 0.0}\n",
      "{'loss': 6.4114, 'grad_norm': 4.124402046203613, 'learning_rate': 5.1111111111111115e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9249, 'grad_norm': 3.9898059368133545, 'learning_rate': 5.037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 4.7508, 'grad_norm': 2.743068218231201, 'learning_rate': 4.962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8384, 'grad_norm': 3.7696239948272705, 'learning_rate': 4.888888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9986, 'grad_norm': 4.254659175872803, 'learning_rate': 4.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4962, 'grad_norm': 3.334810733795166, 'learning_rate': 4.7407407407407415e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9401, 'grad_norm': 3.990863084793091, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3713, 'grad_norm': 3.4022581577301025, 'learning_rate': 4.592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6035, 'grad_norm': 3.8497114181518555, 'learning_rate': 4.5185185185185185e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8795, 'grad_norm': 3.5570859909057617, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.0}\n",
      "{'loss': 5.93, 'grad_norm': 4.035972595214844, 'learning_rate': 4.370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0516, 'grad_norm': 4.05921745300293, 'learning_rate': 4.296296296296296e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9764, 'grad_norm': 3.772160768508911, 'learning_rate': 4.222222222222223e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9738, 'grad_norm': 3.3748414516448975, 'learning_rate': 4.1481481481481485e-06, 'epoch': 0.0}\n",
      "{'loss': 6.2136, 'grad_norm': 3.999753475189209, 'learning_rate': 4.074074074074074e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4391, 'grad_norm': 2.992340564727783, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7702, 'grad_norm': 3.478863477706909, 'learning_rate': 3.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0847, 'grad_norm': 3.7847518920898438, 'learning_rate': 3.851851851851852e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9924, 'grad_norm': 3.8556759357452393, 'learning_rate': 3.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8544, 'grad_norm': 3.3205361366271973, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1714, 'grad_norm': 3.887319803237915, 'learning_rate': 3.6296296296296302e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7103, 'grad_norm': 3.2147443294525146, 'learning_rate': 3.555555555555556e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5731, 'grad_norm': 2.7577788829803467, 'learning_rate': 3.481481481481482e-06, 'epoch': 0.0}\n",
      "{'loss': 5.898, 'grad_norm': 4.1066813468933105, 'learning_rate': 3.4074074074074077e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0247, 'grad_norm': 3.871016263961792, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0922, 'grad_norm': 4.025548934936523, 'learning_rate': 3.25925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8186, 'grad_norm': 3.630462169647217, 'learning_rate': 3.1851851851851855e-06, 'epoch': 0.0}\n",
      "{'loss': 6.0076, 'grad_norm': 3.6633527278900146, 'learning_rate': 3.1111111111111116e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8289, 'grad_norm': 3.505941152572632, 'learning_rate': 3.0370370370370372e-06, 'epoch': 0.0}\n",
      "{'loss': 5.302, 'grad_norm': 3.1208062171936035, 'learning_rate': 2.962962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6574, 'grad_norm': 3.237488269805908, 'learning_rate': 2.888888888888889e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3308, 'grad_norm': 3.0538713932037354, 'learning_rate': 2.814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8899, 'grad_norm': 2.5305652618408203, 'learning_rate': 2.740740740740741e-06, 'epoch': 0.0}\n",
      "{'loss': 5.4036, 'grad_norm': 2.9985098838806152, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8347, 'grad_norm': 3.740753173828125, 'learning_rate': 2.5925925925925925e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9302, 'grad_norm': 3.9635252952575684, 'learning_rate': 2.5185185185185186e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8363, 'grad_norm': 3.515618085861206, 'learning_rate': 2.4444444444444447e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5163, 'grad_norm': 2.8039231300354004, 'learning_rate': 2.3703703703703707e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5471, 'grad_norm': 3.6498289108276367, 'learning_rate': 2.2962962962962964e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5162, 'grad_norm': 3.4615838527679443, 'learning_rate': 2.222222222222222e-06, 'epoch': 0.0}\n",
      "{'loss': 5.7388, 'grad_norm': 3.1166839599609375, 'learning_rate': 2.148148148148148e-06, 'epoch': 0.0}\n",
      "{'loss': 4.8224, 'grad_norm': 2.5952022075653076, 'learning_rate': 2.0740740740740742e-06, 'epoch': 0.0}\n",
      "{'loss': 5.8759, 'grad_norm': 3.30523943901062, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 5.6282, 'grad_norm': 3.076439380645752, 'learning_rate': 1.925925925925926e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3192, 'grad_norm': 2.8034565448760986, 'learning_rate': 1.8518518518518519e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5272, 'grad_norm': 3.195308208465576, 'learning_rate': 1.777777777777778e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3246, 'grad_norm': 2.821117401123047, 'learning_rate': 1.7037037037037038e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5062, 'grad_norm': 3.339019536972046, 'learning_rate': 1.62962962962963e-06, 'epoch': 0.0}\n",
      "{'loss': 5.374, 'grad_norm': 2.916844367980957, 'learning_rate': 1.5555555555555558e-06, 'epoch': 0.0}\n",
      "{'loss': 5.9302, 'grad_norm': 3.1544156074523926, 'learning_rate': 1.4814814814814815e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9498, 'grad_norm': 2.510974645614624, 'learning_rate': 1.4074074074074075e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5681, 'grad_norm': 3.5516762733459473, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.0}\n",
      "{'loss': 4.9176, 'grad_norm': 2.638446807861328, 'learning_rate': 1.2592592592592593e-06, 'epoch': 0.0}\n",
      "{'loss': 5.738, 'grad_norm': 3.412961483001709, 'learning_rate': 1.1851851851851854e-06, 'epoch': 0.0}\n",
      "{'loss': 4.6651, 'grad_norm': 2.42824387550354, 'learning_rate': 1.111111111111111e-06, 'epoch': 0.0}\n",
      "{'loss': 5.5436, 'grad_norm': 3.3300085067749023, 'learning_rate': 1.0370370370370371e-06, 'epoch': 0.0}\n",
      "{'loss': 5.3414, 'grad_norm': 2.947559118270874, 'learning_rate': 9.62962962962963e-07, 'epoch': 0.0}\n",
      "{'loss': 5.0102, 'grad_norm': 2.7362563610076904, 'learning_rate': 8.88888888888889e-07, 'epoch': 0.0}\n",
      "{'loss': 5.3019, 'grad_norm': 2.9919047355651855, 'learning_rate': 8.14814814814815e-07, 'epoch': 0.0}\n",
      "{'loss': 5.2664, 'grad_norm': 2.878725290298462, 'learning_rate': 7.407407407407407e-07, 'epoch': 0.0}\n",
      "{'loss': 5.3651, 'grad_norm': 2.5640408992767334, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.0}\n",
      "{'loss': 5.7232, 'grad_norm': 3.2087221145629883, 'learning_rate': 5.925925925925927e-07, 'epoch': 0.0}\n",
      "{'loss': 5.5852, 'grad_norm': 3.0045506954193115, 'learning_rate': 5.185185185185186e-07, 'epoch': 0.0}\n",
      "{'loss': 5.7187, 'grad_norm': 3.440906286239624, 'learning_rate': 4.444444444444445e-07, 'epoch': 0.0}\n",
      "{'loss': 5.2881, 'grad_norm': 3.2752785682678223, 'learning_rate': 3.7037037037037036e-07, 'epoch': 0.0}\n",
      "{'loss': 5.4289, 'grad_norm': 2.7218427658081055, 'learning_rate': 2.9629629629629634e-07, 'epoch': 0.0}\n",
      "{'loss': 5.4034, 'grad_norm': 3.203244686126709, 'learning_rate': 2.2222222222222224e-07, 'epoch': 0.0}\n",
      "{'loss': 5.5927, 'grad_norm': 3.2699527740478516, 'learning_rate': 1.4814814814814817e-07, 'epoch': 0.0}\n",
      "{'train_runtime': 13794.3741, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.011, 'train_loss': 6.720036528905233, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅█▆▄█▃█▆▄▇▇▅▇▆▇▇▅▆▅▄▂▃▄▃▃▃▄▂▂▃▃▃▂▂▃▃▃▁▂▂</td></tr><tr><td>train/learning_rate</td><td>▂▄▅▇███▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▇▄▆▅▆▇▅▆▅▆▅▆▅▅▄▅▄▄▃▂▃▂▂▃▃▂▂▂▃▂▂▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1131677770655232.0</td></tr><tr><td>train/epoch</td><td>0.00115</td></tr><tr><td>train/global_step</td><td>150</td></tr><tr><td>train/grad_norm</td><td>3.26995</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>5.5927</td></tr><tr><td>train_loss</td><td>6.72004</td></tr><tr><td>train_runtime</td><td>13794.3741</td></tr><tr><td>train_samples_per_second</td><td>0.174</td></tr><tr><td>train_steps_per_second</td><td>0.011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-frog-16</strong> at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset/runs/gp12ento</a><br/> View project at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20SFW%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241129_020131-gp12ento\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-2b-it-abliterated on CookieBaker SFW Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "trainer_sfw.train()\n",
    "trainer_sfw.model.save_pretrained(\"../models/SFW/checkpoint-150\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_sfw = trainer_sfw.model.merge_and_unload()\n",
    "merged_model_sfw.save_pretrained(\"../models/SFW_merged\")\n",
    "tokenizer.save_pretrained(\"../models/SFW_merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4511bfe5c5234ddf969b31741112b830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/899457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "del trainer_sfw\n",
    "\n",
    "trainer_nsfw = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_nsfw,\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=16,\n",
    "        warmup_steps=15,\n",
    "        max_steps=150,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=True,\n",
    "        bf16=False,\n",
    "        logging_steps=1,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        output_dir=\"../models/NSFW\",\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=150\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe5013f3f1b40dd9b7f81540772bcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\felip\\OneDrive\\Documentos\\GitHub\\CookieBaker-AI\\training\\notebooks\\wandb\\run-20241128_034550-gkoeoj5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e' target=\"_blank\">dutiful-snowflake-2</a></strong> to <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07af52739017405288aaa0840e384ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\Users\\felip\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.0472, 'grad_norm': 5.640249252319336, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9103, 'grad_norm': 5.3359575271606445, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9043, 'grad_norm': 5.645761966705322, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 10.6738, 'grad_norm': 6.25145149230957, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 9.0942, 'grad_norm': 4.865102767944336, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
      "{'loss': 10.5909, 'grad_norm': 6.099635601043701, 'learning_rate': 9.894736842105264e-06, 'epoch': 0.0}\n",
      "{'loss': 10.7541, 'grad_norm': 6.482822418212891, 'learning_rate': 9.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 10.1237, 'grad_norm': 6.0161590576171875, 'learning_rate': 9.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 9.6602, 'grad_norm': 5.7317585945129395, 'learning_rate': 9.578947368421054e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3626, 'grad_norm': 3.9530227184295654, 'learning_rate': 9.473684210526315e-06, 'epoch': 0.0}\n",
      "{'loss': 10.7523, 'grad_norm': 6.8787522315979, 'learning_rate': 9.36842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9793, 'grad_norm': 6.155326843261719, 'learning_rate': 9.263157894736842e-06, 'epoch': 0.0}\n",
      "{'loss': 10.1341, 'grad_norm': 6.502983093261719, 'learning_rate': 9.157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 9.0086, 'grad_norm': 5.242326259613037, 'learning_rate': 9.05263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 9.5701, 'grad_norm': 5.857893943786621, 'learning_rate': 8.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 8.7513, 'grad_norm': 5.6017560958862305, 'learning_rate': 8.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 9.7836, 'grad_norm': 6.31508731842041, 'learning_rate': 8.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 9.8008, 'grad_norm': 6.612189292907715, 'learning_rate': 8.631578947368422e-06, 'epoch': 0.0}\n",
      "{'loss': 9.7586, 'grad_norm': 6.52145528793335, 'learning_rate': 8.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 6.328, 'grad_norm': 3.751744508743286, 'learning_rate': 8.421052631578948e-06, 'epoch': 0.0}\n",
      "{'loss': 9.9537, 'grad_norm': 7.247509479522705, 'learning_rate': 8.315789473684212e-06, 'epoch': 0.0}\n",
      "{'loss': 8.7607, 'grad_norm': 6.077712535858154, 'learning_rate': 8.210526315789475e-06, 'epoch': 0.0}\n",
      "{'loss': 10.0158, 'grad_norm': 7.250381946563721, 'learning_rate': 8.105263157894736e-06, 'epoch': 0.0}\n",
      "{'loss': 8.9885, 'grad_norm': 6.287909507751465, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3258, 'grad_norm': 5.741156101226807, 'learning_rate': 7.894736842105265e-06, 'epoch': 0.0}\n",
      "{'loss': 9.4265, 'grad_norm': 6.8032002449035645, 'learning_rate': 7.789473684210526e-06, 'epoch': 0.0}\n",
      "{'loss': 8.6462, 'grad_norm': 6.104853630065918, 'learning_rate': 7.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5549, 'grad_norm': 5.974133014678955, 'learning_rate': 7.578947368421054e-06, 'epoch': 0.0}\n",
      "{'loss': 8.8603, 'grad_norm': 6.735897064208984, 'learning_rate': 7.473684210526316e-06, 'epoch': 0.0}\n",
      "{'loss': 9.3028, 'grad_norm': 7.271637439727783, 'learning_rate': 7.368421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2866, 'grad_norm': 4.994191646575928, 'learning_rate': 7.263157894736843e-06, 'epoch': 0.0}\n",
      "{'loss': 8.9425, 'grad_norm': 6.951260089874268, 'learning_rate': 7.157894736842106e-06, 'epoch': 0.0}\n",
      "{'loss': 8.1539, 'grad_norm': 6.145110607147217, 'learning_rate': 7.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 8.2207, 'grad_norm': 6.2005934715271, 'learning_rate': 6.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 9.2947, 'grad_norm': 7.999884128570557, 'learning_rate': 6.842105263157896e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5838, 'grad_norm': 5.5475993156433105, 'learning_rate': 6.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0581, 'grad_norm': 6.17701530456543, 'learning_rate': 6.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3187, 'grad_norm': 6.566706657409668, 'learning_rate': 6.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5625, 'grad_norm': 6.761594295501709, 'learning_rate': 6.421052631578948e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8564, 'grad_norm': 5.905500888824463, 'learning_rate': 6.31578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 8.484, 'grad_norm': 7.402847766876221, 'learning_rate': 6.2105263157894745e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6174, 'grad_norm': 5.558272361755371, 'learning_rate': 6.105263157894738e-06, 'epoch': 0.0}\n",
      "{'loss': 8.4544, 'grad_norm': 6.783658981323242, 'learning_rate': 6e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5365, 'grad_norm': 6.750698089599609, 'learning_rate': 5.8947368421052634e-06, 'epoch': 0.0}\n",
      "{'loss': 8.5562, 'grad_norm': 7.054347038269043, 'learning_rate': 5.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3933, 'grad_norm': 7.174205303192139, 'learning_rate': 5.68421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6301, 'grad_norm': 6.223912715911865, 'learning_rate': 5.578947368421052e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9856, 'grad_norm': 6.72188663482666, 'learning_rate': 5.4736842105263165e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0845, 'grad_norm': 7.234642505645752, 'learning_rate': 5.36842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9109, 'grad_norm': 6.163455963134766, 'learning_rate': 5.263157894736842e-06, 'epoch': 0.0}\n",
      "{'loss': 8.3442, 'grad_norm': 7.36458158493042, 'learning_rate': 5.157894736842106e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6125, 'grad_norm': 6.300682067871094, 'learning_rate': 5.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 7.8155, 'grad_norm': 6.794142246246338, 'learning_rate': 4.947368421052632e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7995, 'grad_norm': 7.065849781036377, 'learning_rate': 4.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 7.5334, 'grad_norm': 6.30277156829834, 'learning_rate': 4.736842105263158e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7645, 'grad_norm': 6.848322868347168, 'learning_rate': 4.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4624, 'grad_norm': 6.424248218536377, 'learning_rate': 4.526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1586, 'grad_norm': 5.770240783691406, 'learning_rate': 4.4210526315789476e-06, 'epoch': 0.0}\n",
      "{'loss': 8.0039, 'grad_norm': 6.985846042633057, 'learning_rate': 4.315789473684211e-06, 'epoch': 0.0}\n",
      "{'loss': 7.13, 'grad_norm': 5.9023051261901855, 'learning_rate': 4.210526315789474e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7187, 'grad_norm': 6.839781284332275, 'learning_rate': 4.105263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0968, 'grad_norm': 5.6195292472839355, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2991, 'grad_norm': 6.225754261016846, 'learning_rate': 3.894736842105263e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3844, 'grad_norm': 5.980795383453369, 'learning_rate': 3.789473684210527e-06, 'epoch': 0.0}\n",
      "{'loss': 7.9404, 'grad_norm': 7.404062747955322, 'learning_rate': 3.6842105263157896e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4668, 'grad_norm': 6.115976333618164, 'learning_rate': 3.578947368421053e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4507, 'grad_norm': 5.997700214385986, 'learning_rate': 3.473684210526316e-06, 'epoch': 0.0}\n",
      "{'loss': 7.207, 'grad_norm': 5.956457614898682, 'learning_rate': 3.368421052631579e-06, 'epoch': 0.0}\n",
      "{'loss': 7.7513, 'grad_norm': 7.1224517822265625, 'learning_rate': 3.2631578947368423e-06, 'epoch': 0.0}\n",
      "{'loss': 7.4178, 'grad_norm': 6.216053009033203, 'learning_rate': 3.157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3703, 'grad_norm': 6.576999187469482, 'learning_rate': 3.052631578947369e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8553, 'grad_norm': 5.528324127197266, 'learning_rate': 2.9473684210526317e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0177, 'grad_norm': 5.759592533111572, 'learning_rate': 2.842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 7.3719, 'grad_norm': 6.481958866119385, 'learning_rate': 2.7368421052631583e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9652, 'grad_norm': 6.437648773193359, 'learning_rate': 2.631578947368421e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0061, 'grad_norm': 6.282752990722656, 'learning_rate': 2.5263157894736844e-06, 'epoch': 0.0}\n",
      "{'loss': 6.9764, 'grad_norm': 6.8480095863342285, 'learning_rate': 2.4210526315789477e-06, 'epoch': 0.0}\n",
      "{'loss': 7.6782, 'grad_norm': 7.648647785186768, 'learning_rate': 2.3157894736842105e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2407, 'grad_norm': 6.305149078369141, 'learning_rate': 2.2105263157894738e-06, 'epoch': 0.0}\n",
      "{'loss': 6.1558, 'grad_norm': 4.783631801605225, 'learning_rate': 2.105263157894737e-06, 'epoch': 0.0}\n",
      "{'loss': 7.1595, 'grad_norm': 5.74940824508667, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2115, 'grad_norm': 5.990188121795654, 'learning_rate': 1.8947368421052634e-06, 'epoch': 0.0}\n",
      "{'loss': 6.6953, 'grad_norm': 5.920891284942627, 'learning_rate': 1.7894736842105265e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0951, 'grad_norm': 6.066863536834717, 'learning_rate': 1.6842105263157895e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8988, 'grad_norm': 5.827770709991455, 'learning_rate': 1.5789473684210526e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8437, 'grad_norm': 5.659218788146973, 'learning_rate': 1.4736842105263159e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0453, 'grad_norm': 5.702203750610352, 'learning_rate': 1.3684210526315791e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8899, 'grad_norm': 5.572003364562988, 'learning_rate': 1.2631578947368422e-06, 'epoch': 0.0}\n",
      "{'loss': 6.8307, 'grad_norm': 5.5450544357299805, 'learning_rate': 1.1578947368421053e-06, 'epoch': 0.0}\n",
      "{'loss': 7.2495, 'grad_norm': 5.988315105438232, 'learning_rate': 1.0526315789473685e-06, 'epoch': 0.0}\n",
      "{'loss': 7.0681, 'grad_norm': 6.012745380401611, 'learning_rate': 9.473684210526317e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7353, 'grad_norm': 5.570369720458984, 'learning_rate': 8.421052631578948e-07, 'epoch': 0.0}\n",
      "{'loss': 7.0446, 'grad_norm': 6.087926864624023, 'learning_rate': 7.368421052631579e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7244, 'grad_norm': 5.016287326812744, 'learning_rate': 6.315789473684211e-07, 'epoch': 0.0}\n",
      "{'loss': 7.0284, 'grad_norm': 5.956068515777588, 'learning_rate': 5.263157894736843e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8571, 'grad_norm': 5.817139625549316, 'learning_rate': 4.210526315789474e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8843, 'grad_norm': 5.456579208374023, 'learning_rate': 3.1578947368421055e-07, 'epoch': 0.0}\n",
      "{'loss': 6.7411, 'grad_norm': 5.519063949584961, 'learning_rate': 2.105263157894737e-07, 'epoch': 0.0}\n",
      "{'loss': 7.2883, 'grad_norm': 6.552621841430664, 'learning_rate': 1.0526315789473685e-07, 'epoch': 0.0}\n",
      "{'loss': 6.8105, 'grad_norm': 5.3651347160339355, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 12707.3864, 'train_samples_per_second': 0.126, 'train_steps_per_second': 0.008, 'train_loss': 8.078502898216248, 'epoch': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▄▃▅▅▁▅▅▃▄▄▇▅▅▅▄▅█▆▆▄▆▆▇▆▆▅▄▄▅▅▇▅▂▄▄▄▅▅▅▃</td></tr><tr><td>train/learning_rate</td><td>▄▅████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▇▇█▃▇▇▅▇▆▅▆▃▄▄▅▅▅▅▄▃▃▄▃▃▃▃▃▃▂▂▂▁▃▂▂▃▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>630481593272832.0</td></tr><tr><td>train/epoch</td><td>0.00178</td></tr><tr><td>train/global_step</td><td>100</td></tr><tr><td>train/grad_norm</td><td>5.36513</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>6.8105</td></tr><tr><td>train_loss</td><td>8.0785</td></tr><tr><td>train_runtime</td><td>12707.3864</td></tr><tr><td>train_samples_per_second</td><td>0.126</td></tr><tr><td>train_steps_per_second</td><td>0.008</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dutiful-snowflake-2</strong> at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset/runs/gkoeoj5e</a><br/> View project at: <a href='https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset' target=\"_blank\">https://wandb.ai/mekhyw-insper/Fine-tune%20Gemma-2-2b-it-abliterated%20on%20CookieBaker%20NSFW%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241128_034550-gkoeoj5e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    project='Fine-tune Gemma-2-2b-it-abliterated on CookieBaker NSFW Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")\n",
    "\n",
    "trainer_nsfw.train()\n",
    "trainer_nsfw.model.save_pretrained(\"../models/NSFW/checkpoint-150\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model_nsfw = trainer_nsfw.model.merge_and_unload()\n",
    "merged_model_nsfw.save_pretrained(\"../models/NSFW_merged\")\n",
    "tokenizer.save_pretrained(\"../models/NSFW_merged\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
